<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RVTWV1VZXC"></script>
    <script data-cfasync="false">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-RVTWV1VZXC');</script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="turbo-cache-control" content="no-cache" data-turbo-track="reload" data-track-token="3.6.0.783258083307">

    <!-- See retype.com -->
    <meta name="generator" content="Retype 3.6.0">

    <!-- Primary Meta Tags -->
    <title>Gaussian Mixture Model Clearly Explained  | Ransaka&#39;s Blog</title>
    <meta name="title" content="Gaussian Mixture Model Clearly Explained  | Ransaka's Blog">
    <meta name="description" content="When we talk about Gaussian Mixture Model (later, this will be denoted as GMM in this article), it's essential to know how the KMeans algorithm works.">

    <!-- Canonical -->
    <link rel="canonical" href="https://ransaka.github.io/ransakas-blog/gmm-from-scratch/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://ransaka.github.io/ransakas-blog/gmm-from-scratch/">
    <meta property="og:title" content="Gaussian Mixture Model Clearly Explained  | Ransaka's Blog">
    <meta property="og:description" content="When we talk about Gaussian Mixture Model (later, this will be denoted as GMM in this article), it's essential to know how the KMeans algorithm works.">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://ransaka.github.io/ransakas-blog/gmm-from-scratch/">
    <meta property="twitter:title" content="Gaussian Mixture Model Clearly Explained  | Ransaka's Blog">
    <meta property="twitter:description" content="When we talk about Gaussian Mixture Model (later, this will be denoted as GMM in this article), it's essential to know how the KMeans algorithm works.">

    <script data-cfasync="false">(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="../../resources/css/retype.css?v=3.6.0.783258083307" rel="stylesheet">

    <script data-cfasync="false" src="../../resources/js/config.js?v=3.6.0.783258083307" data-turbo-eval="false" defer></script>
    <script data-cfasync="false" src="../../resources/js/retype.js?v=3.6.0" data-turbo-eval="false" defer></script>
    <script id="lunr-js" data-cfasync="false" src="../../resources/js/lunr.js?v=3.6.0.783258083307" data-turbo-eval="false" defer></script>
    <script id="prism-js" data-cfasync="false" src="../../resources/js/prism.js?v=3.6.0.783258083307" defer></script>
</head>
<body>
    <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
        <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>
    
        <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
            <div class="container relative flex items-center justify-between pr-6 grow md:justify-start">
                <!-- Mobile menu button skeleton -->
                <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
                <div v-cloak id="docs-sidebar-toggle"></div>
        
                <!-- Logo -->
                <div class="flex items-center justify-between h-full py-2 md:w-75">
                    <div class="flex items-center px-2 md:px-6">
                        <a id="docs-site-logo" href="../../" class="flex items-center leading-snug text-xl">
                            <span class="w-10 mr-2 grow-0 shrink-0 overflow-hidden">
                                <img class="max-h-10 dark:hidden md:inline-block" src="../../images/blog/age_dist.png">
                                <img class="max-h-10 hidden dark:inline-block" src="../../images/blog/age_dist.png">
                            </span>
                            <span class="dark:text-white font-semibold line-clamp-1 md:line-clamp-2">Ransaka&#x27;s Blog</span>
                        </a>
                    </div>
        
                    <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
                </div>
        
                <div class="flex justify-between md:grow">
                    <!-- Top Nav -->
                    <nav class="hidden md:flex">
                        <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
                            
                        </ul>
                    </nav>
        
                    <!-- Header Right Skeleton -->
                    <div v-cloak class="flex justify-end grow skeleton">
        
                        <!-- Search input mock -->
                        <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                            <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                                <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                            </div>
                            <input class="w-full h-10 placeholder-gray-400 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 dark:placeholder-dark-400" style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search">
                        </div>
        
                        <!-- Mobile search button -->
                        <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                        </div>
        
                        <!-- Dark mode switch placeholder -->
                        <div class="w-10 h-10 lg:ml-2"></div>
        
                        <!-- History button -->
                        <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                        </div>
                    </div>
        
                    <div v-cloak class="flex justify-end grow">
                        <div id="docs-mobile-search-button"></div>
                        <doc-search-desktop></doc-search-desktop>
        
                        <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                        <doc-history></doc-history>
                    </div>
                </div>
            </div>
        </header>
    
        <div class="container relative flex bg-white">
            <!-- Sidebar Skeleton -->
            <div v-cloak class="fixed flex flex-col shrink-0 duration-300 ease-in-out bg-gray-100 border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">
            
                <!-- Render this div, if config.showSidebarFilter is `true` -->
                <div class="flex items-center h-16 px-6">
                    <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter">
                </div>
            
                <div class="pl-6 mt-1 mb-4">
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                </div>
            
                <div class="shrink-0 mt-auto bg-transparent dark:border-dark-650">
                    <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                        <span class="text-xs whitespace-nowrap">Powered by</span>
                        <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                    </a>
                </div>
            </div>
            
            <!-- Sidebar component -->
            <doc-sidebar v-cloak>
                <template #sidebar-footer>
                    <div class="shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650">
            
                        <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                            <span class="text-xs whitespace-nowrap">Powered by</span>
                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                        </a>
                    </div>
                </template>
            </doc-sidebar>
    
            <div class="grow min-w-0 dark:bg-dark-850">
                <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
                <div class="flex">
                    <div class="min-w-0 p-4 grow md:px-16">
                        <main class="relative pb-12 lg:pt-2">
                            <div class="docs-markdown" id="docs-content">
                                <!-- Rendered if sidebar right is enabled -->
                                <div id="docs-sidebar-right-toggle"></div>
                                <!-- Page content  -->
<doc-anchor-target id="gaussian-mixture-model-clearly-explained" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gaussian-mixture-model-clearly-explained">#</doc-anchor-trigger>
        <span>Gaussian Mixture Model Clearly Explained</span>
    </h1>
</doc-anchor-target>
<doc-anchor-target id="the-only-guide-you-need-to-learn-everything-about-gmm">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#the-only-guide-you-need-to-learn-everything-about-gmm">#</doc-anchor-trigger>
        <span><em>The only guide you need to learn everything about GMM</em></span>
    </h3>
</doc-anchor-target>
<p>When we talk about Gaussian Mixture Model (later, this will be denoted as GMM in this article), it&#x27;s essential to know how the KMeans algorithm works. Because GMM is quite similar to the KMeans, more likely it&#x27;s a probabilistic version of KMeans. This probabilistic feature allows GMM to be applied to many complex problems that KMeans can&#x27;t fit into.</p>
<p>In summary, KMeans have below limitations,</p>
<ol>
<li>It assumed that the clusters were spherical and equally sized, which is not valid in most real-world scenarios.</li>
<li>It&#x27;s a hard clustering method. Meaning each data point is assigned to a single cluster.</li>
</ol>
<p>Due to these limitations, we should know alternatives for KMeans when working on our machine learning projects. In this article, we will explore one of the best alternatives for KMeans clustering, called the Gaussian Mixture Model.</p>
<p>Throughout this article, we will be covering the below points.</p>
<ol>
<li>How Gaussian Mixture Model (GMM) algorithm works — in plain English.</li>
<li>Mathematics behind GMM.</li>
<li>Implement GMM using Python from scratch.</li>
</ol>
<doc-anchor-target id="how-gaussian-mixture-model-gmm-algorithm-works--in-plain-english">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#how-gaussian-mixture-model-gmm-algorithm-works--in-plain-english">#</doc-anchor-trigger>
        <span>How Gaussian Mixture Model (GMM) algorithm works — in plain English</span>
    </h2>
</doc-anchor-target>
<p>How Gaussian Mixture Model (GMM) algorithm works — in plain English
As I have mentioned earlier, we can call GMM probabilistic KMeans because the starting point and training process of the KMeans and GMM are the same. However, KMeans uses a distance-based approach, and GMM uses a probabilistic approach. There is one primary assumption in GMM: the dataset consists of multiple Gaussians, in other words, a mixture of the gaussian.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">#import requred libraries
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.stats import multivariate_normal
from scipy.stats import norm
import warnings
import random

warnings.filterwarnings('ignore')</code></pre>
</doc-codeblock></div>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python"># Generate some data with multiple modes
data1 = np.random.normal(0, 1, 1000)
data2 = np.random.normal(5, 1, 1000)

# Plot the data using seaborn's distplot function
sns.distplot(data1, kde=True, hist=True, bins=100, color='b', hist_kws={'alpha': 0.5})
sns.distplot(data2, kde=True, hist=True, bins=100, color='r', hist_kws={'alpha': 0.5})

# Add a legend
plt.legend(['Data 1', 'Data 2'])

# Show the plot
plt.show()</code></pre>
</doc-codeblock></div>
<p><figure class="content-center">
    <img src="images/gmm/output_6_0.png" alt="png">
    <figcaption class="caption">png</figcaption>
</figure>
</p>
<p>The above kind of distribution is often called multi-model distribution. Each peak represents the different gaussian distribution or the cluster in our dataset. But the question is,</p>
<doc-anchor-target id="how-do-we-estimate-these-distributions">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#how-do-we-estimate-these-distributions">#</doc-anchor-trigger>
        <span><em>how do we estimate these distributions?</em></span>
    </h4>
</doc-anchor-target>
<p>Before answering this question, let&#x27;s create some gaussian distribution first. Please note here I am generating multivariate normal distribution; it&#x27;s a higher dimensional extension of the univariate normal distribution.</p>
<p>Let&#x27;s define the mean and covariance of our data points. Using mean and covariance, we can generate the distribution as follows.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python"># Set the mean and covariance
mean1 = [0, 0]
mean2 = [2, 0]
cov1 = [[1, .7], [.7, 1]]
cov2 = [[.5, .4], [.4, .5]]

# Generate data from the mean and covariance
data1 = np.random.multivariate_normal(mean1, cov1, size=1000)
data2 = np.random.multivariate_normal(mean2, cov2, size=1000)</code></pre>
</doc-codeblock></div>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">plt.figure(figsize=(10,6))

plt.scatter(data1[:,0],data1[:,1])
plt.scatter(data2[:,0],data2[:,1])

sns.kdeplot(data1[:, 0], data1[:, 1], levels=20, linewidth=10, color='k', alpha=0.2)
sns.kdeplot(data2[:, 0], data2[:, 1], levels=20, linewidth=10, color='k', alpha=0.2)

plt.grid(False)
plt.show()</code></pre>
</doc-codeblock></div>
<p><figure class="content-center">
    <img src="images/gmm/output_9_0.png" alt="png">
    <figcaption class="caption">png</figcaption>
</figure>
</p>
<p>As you can see here, we generated random gaussian distribution using mean and covariance matrices. What about reversing this process? That&#x27;s what exactly GMM is doing. But how?</p>
<p><em>Because, in the beginning, we didn’t have any insights about clusters nor their associated mean and covariance matrices</em></p>
<p>Well, It happens according to the below steps,</p>
<ol>
<li>Decide the number of clusters (to decide this, we can use domain knowledge or other methods such as BIC/AIC) for the given dataset. Assume that we have 1000 data points, and we set the number of groups as 2.</li>
<li>Initiate mean, covariance, and weight parameter per cluster. (we will explore more about this in a later section)</li>
<li>Use the Expectation Maximization algorithm to do the following,</li>
</ol>
<ul>
<li>Expectation Step (E step): Calculate the probability of each data point belonging to each data point, then evaluate the likelihood function using the current estimate for the parameters</li>
<li>Maximization step (M step): Update the previous mean, covariance, and weight parameters to maximize the expected likelihood found in the E step</li>
<li>Repeat these steps until the model converges.</li>
</ul>
<p>With this information, I am concluding the no-math explanation of the GMM algorithm.</p>
<doc-anchor-target id="mathematics-behind-gmm">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#mathematics-behind-gmm">#</doc-anchor-trigger>
        <span>Mathematics behind GMM</span>
    </h2>
</doc-anchor-target>
<p>The core of GMM lies within Expectation Maximization(EM) algorithm described in the previous section.</p>
<p>Let&#x27;s demonstrate the EM algorithm in the sense of GMM.</p>
<p><strong>Step 01: Initialize mean, covariance and weight parameters</strong></p>
<ol>
<li>mean (μ): initialize randomly.</li>
<li>covariance (Σ): initialize randomly</li>
<li>weight (mixing coefficients) (π): fraction per class refers to the likelihood that a particular data point belongs to each class. In the beginning, this will be equal for all clusters. Assume that we fit a GMM with three components. In this case weight parameter might be set to 1/3 for each component, resulting in a probability distribution of (1/3, 1/3, 1/3).</li>
</ol>
<p><strong>Step 02: Expectation Step (E step)</strong></p>
<p>For each data point 𝑥𝑖:
Calculate the probability that the data point belongs to cluster (𝑐) using the below equation. k is the number of distributions we are supposed to find.</p>
<p><figure class="content-center">
    <img src="images/gmm/eq1.png" alt="images/eq1.png">
    <figcaption class="caption">images/eq1.png</figcaption>
</figure>
</p>
<p>Where 𝜋_𝑐 is the mixing coefficient (sometimes called weight) for the Gaussian distribution c, which was initialized in the previous stage, and 𝑁(𝒙 | 𝝁,𝚺) describes the probability density function (PDF) of a Gaussian distribution with mean 𝜇 and covariance Σ with respect to data point x; We can denote it as below.</p>
<p><figure class="content-center">
    <img src="images/gmm/eq2.png" alt="eq2.png">
    <figcaption class="caption">eq2.png</figcaption>
</figure>
</p>
<p>The E-step computes these probabilities using the current estimates of the model&#x27;s parameters. These probabilities are typically referred to as the &quot;responsibilities&quot; of the Gaussian distributions. They are represented by the variables r_ic, where i is the index of the data point, and c is the index of the Gaussian distribution. The responsibility measures how much the c-th Gaussian distribution is responsible for generating the i-th data point. Conditional probability is used here, more specifically, Bayes theorem.</p>
<p>Let&#x27;s take a simple example. Assume we have 100 data points and need to cluster them into two groups. We can write r_ic(i=20,c=1) as follows. Where i represents the data point&#x27;s index, and c represents the index of the cluster we are considering.</p>
<p>Please note at the beginning, 𝜋_𝑐 initialized to equal for each cluster c = 1,2,3,..,k. In our case, 𝜋_1 = 𝜋_2 = 1/2.</p>
<p><figure class="content-center">
    <img src="images/gmm/image_3.png" alt="image.png">
    <figcaption class="caption">image.png</figcaption>
</figure>
</p>
<p>The result of the E-step is a set of responsibilities for each data point and each Gaussian distribution in the mixture model. These responsibilities are used in the M-step to update the estimates of the model&#x27;s parameters.</p>
<p><strong>Step 03: Maximization Step (M step)</strong></p>
<p>In this step, the algorithm uses the responsibilities of the Gaussian distributions (computed in the E-step) to update the estimates of the model&#x27;s parameters.</p>
<p>The M-step updates the estimates of the parameters as follows:</p>
<p><figure class="content-center">
    <img src="images/gmm/equatios3.png" alt="equatios3.png">
    <figcaption class="caption">equatios3.png</figcaption>
</figure>
</p>
<ol>
<li><p>Update the πc (mixing coefficients) using equation 4 above.</p>
</li>
<li><p>Update the μc using equation number 5 above.</p>
</li>
<li><p>Then update the Σc using the 6th equation.</p>
</li>
</ol>
<p>Additional Fact:</p>
<p><em>πc can be considered equivalent to the fraction of points allocated to 𝑐 because numerator Σ_𝑖 *𝑟_𝑖𝑐 represents the likelihood of the data point belonging to the gaussian c. If we assume we have 3 clusters and 𝑖-th data point belongs to cluster 1, we can write the related vector as [0.97,0.02,0.01]. If we sum these vectors for each data point, the result vector is approximately equal to the number of data points per cluster.</em></p>
<p>This updated estimate is used in the next E-step to compute new responsibilities for the data points.</p>
<p>So on and so forth, this process will repeat until algorithm convergence, typically achieved when the model parameters do not change significantly from one iteration to the next.</p>
<p>Lots of ugly and complex equations, right? :)</p>
<doc-anchor-target id="lets-summarize-the-above-facts-into-one-simple-diagram">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#lets-summarize-the-above-facts-into-one-simple-diagram">#</doc-anchor-trigger>
        <span><em>Let’s summarize the above facts into one simple diagram,</em></span>
    </h3>
</doc-anchor-target>
<p><figure class="content-center">
    <img src="images/gmm/summary.png" alt="summary.png">
    <figcaption class="caption">summary.png</figcaption>
</figure>
</p>
<p>Don&#x27;t worry; when it comes to coding, it will be one line per each equation. Let&#x27;s start to implement GMM from scratch using Python.</p>
<doc-anchor-target id="implement-gmm-using-python-from-scratch">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#implement-gmm-using-python-from-scratch">#</doc-anchor-trigger>
        <span>Implement GMM using Python from scratch.</span>
    </h2>
</doc-anchor-target>
<p><figure class="content-center">
    <img src="images/gmm/animated_gmm20new.gif" alt="animated_GMM new.gif">
    <figcaption class="caption">animated_GMM new.gif</figcaption>
</figure>
</p>
<p>First thing first, let&#x27;s create a fake dataset. In this section, I will implement GMM for the 1-D dataset.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">n_samples = 100
mu1, sigma1 = -5, 1.2 
mu2, sigma2 = 5, 1.8 
mu3, sigma3 = 0, 1.6 

x1 = np.random.normal(loc = mu1, scale = np.sqrt(sigma1), size = n_samples)
x2 = np.random.normal(loc = mu2, scale = np.sqrt(sigma2), size = n_samples)
x3 = np.random.normal(loc = mu3, scale = np.sqrt(sigma3), size = n_samples)

X = np.concatenate((x1,x2,x3))</code></pre>
</doc-codeblock></div>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">def plot_pdf(mu,sigma,label,alpha=0.5,linestyle='k--',density=True,color='green'):
    &quot;&quot;&quot;
    Plot 1-D data and its PDF curve.

    Parameters
    ----------
    X : array-like, shape (n_samples,)
        The input data.
    &quot;&quot;&quot;
    # Compute the mean and standard deviation of the data

    # Plot the data
    
    X = norm.rvs(mu, sigma, size=1000)
    
    plt.hist(X, bins=50, density=density, alpha=alpha,label=label,color=color)

    # Plot the PDF
    x = np.linspace(X.min(), X.max(), 1000)
    y = norm.pdf(x, mu, sigma)
    plt.plot(x, y, linestyle)
</code></pre>
</doc-codeblock></div>
<p>And plot the generated data as follows. Please note that instead of plotting the data itself, I have plotted the probability density of each sample.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">plot_pdf(mu1,sigma1,label=r&quot;$\mu={} \ ; \ \sigma={}$&quot;.format(mu1,sigma1),color=None)
plot_pdf(mu2,sigma2,label=r&quot;$\mu={} \ ; \ \sigma={}$&quot;.format(mu2,sigma2),color=None)
plot_pdf(mu3,sigma3,label=r&quot;$\mu={} \ ; \ \sigma={}$&quot;.format(mu3,sigma3),color=None)
plt.title(&quot;Original Distribution&quot;)
plt.legend()
plt.show()</code></pre>
</doc-codeblock></div>
<p><figure class="content-center">
    <img src="images/gmm/output_39_0.png" alt="png">
    <figcaption class="caption">png</figcaption>
</figure>
</p>
<p>Let&#x27;s build each step described in the previous section,</p>
<p><strong>Step 01: Initialize mean, covariance, and weights</strong></p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">def random_init(n_compenents):
    
    &quot;&quot;&quot;Initialize means, weights and variance randomly&quot;&quot;&quot;
    
    pi = np.ones((n_compenents)) / n_compenents
    means = np.random.choice(X, n_compenents)
    variances = np.random.random_sample(size=n_compenents)
    plot_pdf(means[0],variances[0],'Random Init 01',)
    plot_pdf(means[1],variances[1],'Random Init 02',color='blue')
    plot_pdf(means[2],variances[2],'Random Init 03',color='orange')
    
    plt.title(&quot;Random Initialization&quot;)
    
    plt.legend()
    plt.show()
    
    return means,variances,pi</code></pre>
</doc-codeblock></div>
<p><strong>Step 02: Expectation Step (E step)</strong></p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">def step_expectation(X,n_components,means,variances):
    &quot;&quot;&quot;E Step
    
    Parameters
    ----------
    X : array-like, shape (n_samples,)
        The data.
    n_components : int
        The number of clusters
    means : array-like, shape (n_components,)
        The means of each mixture component.
    variances : array-like, shape (n_components,)
        The variances of each mixture component.
        
    Returns
    -------
    weights : array-like, shape (n_components,n_samples)
    &quot;&quot;&quot;
    weights = np.zeros((n_components,len(X)))
    for j in range(n_components):
        weights[j,:] = norm(loc=means[j],scale=np.sqrt(variances[j])).pdf(X)
    return weights</code></pre>
</doc-codeblock></div>
<p>After this function, we covered the first two equations we discussed in E Step. Here we have generated the gaussian distribution for the current model parameter means and variances. We accomplished that by using the scipy&#x27;s stat module. After, we used the pdf method to calculate the likelihood of belonging to each data point for each cluster.</p>
<p><strong>Step 03: Maximization Step (M step)</strong></p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">def step_maximization(X,weights,means,variances,n_compenents,pi):
    &quot;&quot;&quot;M Step
    
    Parameters
    ----------
    X : array-like, shape (n_samples,)
        The data.
    weights : array-like, shape (n_components,n_samples)
        initilized weights array
    means : array-like, shape (n_components,)
        The means of each mixture component.
    variances : array-like, shape (n_components,)
        The variances of each mixture component.
    n_components : int
        The number of clusters
    pi: array-like (n_components,)
        mixture component weights
        
    Returns
    -------
    means : array-like, shape (n_components,)
        The means of each mixture component.
    variances : array-like, shape (n_components,)
        The variances of each mixture component.
    &quot;&quot;&quot;
    r = []
    for j in range(n_compenents):  
        r.append((weights[j] * pi[j]) / (np.sum([weights[i] * pi[i] for i in range(n_compenents)], axis=0)))

        means[j] = np.sum(r[j] * X) / (np.sum(r[j]))
        variances[j] = np.sum(r[j] * np.square(X - means[j])) / (np.sum(r[j]))
    
        pi[j] = np.mean(r[j])

    return variances,means,pi</code></pre>
</doc-codeblock></div>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">def plot_intermediate_steps(means,variances,density=False,save=False,file_name=None):
    
    plot_pdf(mu1,sigma1,alpha=0.0,linestyle='r--',label='Original Distibutions')
    plot_pdf(mu2,sigma2,alpha=0.0,linestyle='r--',label='Original Distibutions')
    plot_pdf(mu3,sigma3,alpha=0.0,linestyle='r--',label='Original Distibutions')
    
    color_gen = (x for x in ['green','blue','orange'])
    
    for mu,sigma in zip(means,variances):
        plot_pdf(mu,sigma,alpha=0.5,label='d',color=next(color_gen))
    if save or file_name is not None:
        step = file_name.split(&quot;_&quot;)[1]
        plt.title(f&quot;step: {step}&quot;)
        plt.savefig(f&quot;steps/{file_name}.png&quot;,bbox_inches='tight')
    plt.show()</code></pre>
</doc-codeblock></div>
<p>Let&#x27;s implement the training loop.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">def train_gmm(data,n_compenents=3,n_steps=50, plot_intermediate_steps_flag=True):
    &quot;&quot;&quot; Training step of the GMM model
    
    Parameters
    ----------
    data : array-like, shape (n_samples,)
        The data.
    n_components : int
        The number of clusters
    n_steps: int
        number of iterations to run
    &quot;&quot;&quot;
    
    
    means,variances,pi = random_init(n_compenents)
    for step in range(n_steps):
        weights = step_expectation(data,n_compenents,means,variances)
        variances,means,pi = step_maximization(X, weights, means, variances, n_compenents, pi)
        if plot_intermediate_steps_flag:plot_intermediate_steps(means,variances,)#file_name=f'step_{step+1}')
    plot_intermediate_steps(means,variances)</code></pre>
</doc-codeblock></div>
<p>When we start the model training, we will do E and M steps according to the n_steps parameter we set.</p>
<p>But in the actual use cases, you will use the scikit-learn version of the GMM more often. There you can find additional parameters, such as</p>
<p>tol: defining the model’s stop criteria. EM iterations will stop when the lower bound average gain is below the tol parameter.</p>
<p>init_params: The method used to initialize the weights</p>
<p>You may refer to the documentation <a href="https://scikitlearn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html.">here</a></p>
<p>Alright, let&#x27;s see how our handcrafted GMM performs.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">train_gmm(X,n_steps=30,plot_intermediate_steps_flag=True)</code></pre>
</doc-codeblock></div>
<p><figure class="content-center">
    <img src="images/gmm/model20traininng.png" alt="model traininng.png">
    <figcaption class="caption">model traininng.png</figcaption>
</figure>
</p>
<p>In the above diagrams, red dashed lines represent the original distribution, while other graphs represent the learned distributions. After the 30th iteration, we can see that our model performed well on this toy dataset.</p>

                                
                                <!-- Required only on API pages -->
                                <doc-toolbar-member-filter-no-results></doc-toolbar-member-filter-no-results>
                            </div>
                            <footer class="clear-both">
                            
                                <nav class="flex mt-14">
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 h-full flex items-center break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="../../ransakas-blog/building-crnn-model-for-sinhala-ocr/">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                            <span>
                                                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                                                <span class="block mt-1">Building CRNN model for Sinhala OCR Tasks</span>
                                            </span>
                                        </a>
                                    </div>
                            
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="../../ransakas-blog/machine-learning-on-snowflake/">
                                            <span>
                                                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                                                <span class="block mt-1">Machine Learning on Snowflake</span>
                                            </span>
                                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        </a>
                                    </div>
                                </nav>
                            </footer>
                        </main>
                
                        <div class="border-t dark:border-dark-650 pt-6 mb-8">
                            <footer class="flex flex-wrap items-center justify-between">
                                <div>
                                    <ul class="flex flex-wrap items-center text-sm">
                                    </ul>
                                </div>
                                <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2024. All rights reserved.</p></div>
                            </footer>
                        </div>
                    </div>
                
                    <!-- Rendered if sidebar right is enabled -->
                    <!-- Sidebar right skeleton-->
                    <div v-cloak class="fixed top-0 bottom-0 right-0 translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:shrink-0 lg:pt-6 lg:transform-none sm:w-1/2 lg:w-64 lg:z-0 md:w-104 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
                        <div class="pl-5">
                            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                        </div>
                    </div>
                
                    <!-- User should be able to hide sidebar right -->
                    <doc-sidebar-right v-cloak></doc-sidebar-right>
                </div>

            </div>
        </div>
    
        <doc-search-mobile></doc-search-mobile>
        <doc-back-to-top></doc-back-to-top>
    </div>


    <div id="docs-overlay-target"></div>

    <script data-cfasync="false">window.__DOCS__ = { "title": "Gaussian Mixture Model Clearly Explained", level: 2, icon: "file", hasPrism: true, hasMermaid: false, hasMath: false, tocDepth: 23 }</script>
</body>
</html>
