<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RVTWV1VZXC"></script>
    <script data-cfasync="false">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-RVTWV1VZXC');</script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="turbo-cache-control" content="no-cache" data-turbo-track="reload" data-track-token="3.6.0.783258083307">

    <!-- See retype.com -->
    <meta name="generator" content="Retype 3.6.0">

    <!-- Primary Meta Tags -->
    <title>Building CRNN model for Sinhala OCR&#160;Tasks  | Ransaka&#39;s Blog</title>
    <meta name="title" content="Building CRNN model for Sinhala OCR Tasks  | Ransaka's Blog">
    <meta name="description" content="Welcome">

    <!-- Canonical -->
    <link rel="canonical" href="https://ransaka.github.io/ransakas-blog/building-crnn-model-for-sinhala-ocr/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://ransaka.github.io/ransakas-blog/building-crnn-model-for-sinhala-ocr/">
    <meta property="og:title" content="Building CRNN model for Sinhala OCR Tasks  | Ransaka's Blog">
    <meta property="og:description" content="Welcome">
    <meta property="og:image" content="https://ransaka.github.io/images/crnn-sinhala/output.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://ransaka.github.io/ransakas-blog/building-crnn-model-for-sinhala-ocr/">
    <meta property="twitter:title" content="Building CRNN model for Sinhala OCR Tasks  | Ransaka's Blog">
    <meta property="twitter:description" content="Welcome">
    <meta property="twitter:image" content="https://ransaka.github.io/images/crnn-sinhala/output.png">

    <script data-cfasync="false">(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="../../resources/css/retype.css?v=3.6.0.783258083307" rel="stylesheet">

    <script data-cfasync="false" src="../../resources/js/config.js?v=3.6.0.783258083307" data-turbo-eval="false" defer></script>
    <script data-cfasync="false" src="../../resources/js/retype.js?v=3.6.0" data-turbo-eval="false" defer></script>
    <script id="lunr-js" data-cfasync="false" src="../../resources/js/lunr.js?v=3.6.0.783258083307" data-turbo-eval="false" defer></script>
    <script id="prism-js" data-cfasync="false" src="../../resources/js/prism.js?v=3.6.0.783258083307" defer></script>
</head>
<body>
    <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
        <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>
    
        <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
            <div class="container relative flex items-center justify-between pr-6 grow md:justify-start">
                <!-- Mobile menu button skeleton -->
                <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
                <div v-cloak id="docs-sidebar-toggle"></div>
        
                <!-- Logo -->
                <div class="flex items-center justify-between h-full py-2 md:w-75">
                    <div class="flex items-center px-2 md:px-6">
                        <a id="docs-site-logo" href="../../" class="flex items-center leading-snug text-xl">
                            <span class="w-10 mr-2 grow-0 shrink-0 overflow-hidden">
                                <img class="max-h-10 dark:hidden md:inline-block" src="../../images/blog/age_dist.png">
                                <img class="max-h-10 hidden dark:inline-block" src="../../images/blog/age_dist.png">
                            </span>
                            <span class="dark:text-white font-semibold line-clamp-1 md:line-clamp-2">Ransaka&#x27;s Blog</span>
                        </a>
                    </div>
        
                    <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
                </div>
        
                <div class="flex justify-between md:grow">
                    <!-- Top Nav -->
                    <nav class="hidden md:flex">
                        <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
                            
                        </ul>
                    </nav>
        
                    <!-- Header Right Skeleton -->
                    <div v-cloak class="flex justify-end grow skeleton">
        
                        <!-- Search input mock -->
                        <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                            <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                                <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                            </div>
                            <input class="w-full h-10 placeholder-gray-400 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 dark:placeholder-dark-400" style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search">
                        </div>
        
                        <!-- Mobile search button -->
                        <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                        </div>
        
                        <!-- Dark mode switch placeholder -->
                        <div class="w-10 h-10 lg:ml-2"></div>
        
                        <!-- History button -->
                        <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                        </div>
                    </div>
        
                    <div v-cloak class="flex justify-end grow">
                        <div id="docs-mobile-search-button"></div>
                        <doc-search-desktop></doc-search-desktop>
        
                        <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                        <doc-history></doc-history>
                    </div>
                </div>
            </div>
        </header>
    
        <div class="container relative flex bg-white">
            <!-- Sidebar Skeleton -->
            <div v-cloak class="fixed flex flex-col shrink-0 duration-300 ease-in-out bg-gray-100 border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">
            
                <!-- Render this div, if config.showSidebarFilter is `true` -->
                <div class="flex items-center h-16 px-6">
                    <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter">
                </div>
            
                <div class="pl-6 mt-1 mb-4">
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                </div>
            
                <div class="shrink-0 mt-auto bg-transparent dark:border-dark-650">
                    <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                        <span class="text-xs whitespace-nowrap">Powered by</span>
                        <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                    </a>
                </div>
            </div>
            
            <!-- Sidebar component -->
            <doc-sidebar v-cloak>
                <template #sidebar-footer>
                    <div class="shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650">
            
                        <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                            <span class="text-xs whitespace-nowrap">Powered by</span>
                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                        </a>
                    </div>
                </template>
            </doc-sidebar>
    
            <div class="grow min-w-0 dark:bg-dark-850">
                <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
                <div class="flex">
                    <div class="min-w-0 p-4 grow md:px-16">
                        <main class="relative pb-12 lg:pt-2">
                            <div class="docs-markdown" id="docs-content">
                                <!-- Rendered if sidebar right is enabled -->
                                <div id="docs-sidebar-right-toggle"></div>
                                <!-- Page content  -->
<doc-anchor-target id="building-crnn-model-for-sinhala-ocrtasks" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#building-crnn-model-for-sinhala-ocrtasks">#</doc-anchor-trigger>
        <span>Building CRNN model for Sinhala OCR Tasks</span>
    </h1>
</doc-anchor-target>
<doc-anchor-target id="step-by-step-guide-to-training-own-ocrmodel">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#step-by-step-guide-to-training-own-ocrmodel">#</doc-anchor-trigger>
        <span><em>Step-by-step guide to training own OCR model</em></span>
    </h3>
</doc-anchor-target>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/output.png" alt="Welcome">
    <figcaption class="caption">Welcome</figcaption>
</figure>
</p>
<doc-anchor-target id="opening-thoughts">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#opening-thoughts">#</doc-anchor-trigger>
        <span>Opening thoughts</span>
    </h2>
</doc-anchor-target>
<p>If you&#x27;re reading this article, you likely have different interests than the majority. It&#x27;s rare to find someone wanting to build a CRNN model in this era of LLM hype, which makes this quite refreshing. Next, I want to share my motivation behind this article. However, if you feel that&#x27;s unnecessary, you can directly jump into <doc-anchor-trigger to="#understanding-crnn">this section</doc-anchor-trigger>. So, my motivation behind this article(possibly an article series) is making people want to play around with RNNs. RNNs are the pioneers in this sequence modelling before transformers come into play. Most concepts we are now trying out with transformers have <a href="https://arxiv.org/pdf/1409.0473">evolved from RNNs</a>. Therefore, I believe these concepts with RNNs make much more sense than those with more complex and heavy-weight transformer architecture. As per this thought process, I decided to go ahead with it. While I learn these, I am using these articles to digest these concepts better. Alright, enough talking. Let&#x27;s jump into the actual article.</p>
<doc-anchor-target id="understanding-crnn">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#understanding-crnn">#</doc-anchor-trigger>
        <span>Understanding CRNN</span>
    </h2>
</doc-anchor-target>
<p>CRNN (Convolutional Recurrent Neural Network) was first proposed in the paper <em>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</em>. As the name implies, the architecture comprises a convolutional neural network with an RNN model. But how? Let me explain. First, forget about these terms: RNNs, CNNs, etc. How would you do that if you were supposed to read the text in the image below?</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/1_vbznz8qjxscukul6zak55g.png" alt="Sample Image via MJSynth">
    <figcaption class="caption">Sample Image via MJSynth</figcaption>
</figure>
</p>
<p>It&#x27;s pretty simple, right? All you need to know is to scan the image with your eyes. Once you understand the first character, you shift right (or left, depending on the language). You continue this process until you reach the end of the image. This is exactly what CRNNs do under the hood. The only difference is that you&#x27;ve trained to do this throughout your life, unlike these models, which only have a few days to achieve the same results. ¯/<em>(ツ)</em>/¯</p>
<p>But there is a tiny difference. We humans can understand a character at first glance.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/fig1.png" alt="Fig 1">
    <figcaption class="caption">Fig 1</figcaption>
</figure>
</p>
<p>On the other hand, machines are quite incapable of this as they are limited by capturing only spatial representation, the amount of data they can access at certain points and, more importantly, the nature of data we put in. Intuitively, if we need to build a system capable of predicting corresponding text in certain regions, we should have a properly annotated dataset. One possibility would be to create patches of images and assign characters to each, like this.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/fig2.png" alt="Fig 2 Map different image patches to a label">
    <figcaption class="caption">Fig 2 Map different image patches to a label</figcaption>
</figure>
</p>
<p>In practice, creating this kind of dataset requires some serious manpower and careful annotations. But we can do the trick; what if we allow a CNN model to self-create these patches and then label each patch based on its spatial representation? That might work, isn&#x27;t it? But again, an obstacle: If you check the above image, you will notice that you need both the first and second patches to predict <em>S</em>. The same goes with E and C above. Therefore, we will predict the current label based on information from previous patches and the current patch itself. In other words, it&#x27;s sequence modelling. And that&#x27;s where RNN comes in here. 
So far in the explanation, you have probably noted how we transitioned this OCR task into CNN and RNN. More intuitively, that is why they called this CRNN. It has tackled the OCR problem using a combination of CNN and RNN. CNN to learn spatial features in the image, then pass into RNN to detect the relationship between patches(feature maps) and produce the final output. 
I&#x27;m using <a href="https://huggingface.co/datasets/Ransaka/SSOCR-V.1">this</a> dataset in the later part of this article. It contains synthetic images suitable for OCR tasks. You can access the generation script in <a href="https://github.com/Ransaka/rnn-playground/tree/main/rnn-for-sinhala-ocr">this code repository</a>. Furthermore, all codes related to this article can be accessed from the same GitHub repository.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/1_0ndcabarp3ywwpwcjjy_sg.png" alt="SSOCR Dataset">
    <figcaption class="caption">SSOCR Dataset</figcaption>
</figure>
</p>
<doc-anchor-target id="more-detailed-explanation-about-the-model-architecture">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#more-detailed-explanation-about-the-model-architecture">#</doc-anchor-trigger>
        <span>More detailed explanation about the model architecture.</span>
    </h2>
</doc-anchor-target>
<p>As initially discussed, it has two components: CNN and RNN. However, the original paper stated a third component, the transcription layer. That&#x27;s where we post-process predicted output and generate final predictions. Furthermore, unlike traditional CNN, we don&#x27;t need to use a fully connected layer as the final layer, as final feature maps are transformed into feature vectors and then passed to RNN. This results in many-to-one mapping between feature vectors and corresponding labels. In the dataset, it&#x27;s just an image, and text appears (See Fig 2). Hence, the model is supposed to align the text with a sequence of feature vectors by itself.</p>
<doc-anchor-target id="feature-vectors-feature-maps-and-imagepatches">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#feature-vectors-feature-maps-and-imagepatches">#</doc-anchor-trigger>
        <span>Feature vectors, feature maps and image patches</span>
    </h2>
</doc-anchor-target>
<p>Perhaps you needed clarification on these three, as I often used them in the explanation. Here&#x27;s the explanation. Image patches are horizontal or vertical (or a combination of both) image segments, as shown in Fig. That has nothing to do with this CRNN model. I only used it in the article to get the reader used to the concept of CRNN. However, feature maps and feature vectors deserve more explanation because they are major components of CRNN architecture.</p>
<doc-anchor-target id="feature-maps">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#feature-maps">#</doc-anchor-trigger>
        <span>Feature maps</span>
    </h2>
</doc-anchor-target>
<p><a href="https://stats.stackexchange.com/questions/291820/what-is-the-definition-of-a-feature-map-aka-activation-map-in-a-convolutio">Feature maps</a> are the product of CNN. When the image passes through convolution and pooling layers, it transforms into several feature maps. Each convolution layer applies a different filter, and when we perform this for multiple layers, we have a very abstract view of the input image. A pooling layer is also applied between convolution layers to retain the most usable feature. This will reduce the resulting feature map dimension. Ultimately, the resulting feature maps can be considered feature-rich and efficient  representations of the input image. Due to dimensionality reduction, a small area in a resulting feature map corresponds to a larger area in the input image. We call that receptive field. Nevertheless, to understand feature maps in the context of CRNN, all you need to know is that feature maps have very specialized representations of input features, which we transform as feature vectors.</p>
<p>Let&#x27;s assume we have a 61 x 157 image. Our model comprises two convolution and max pooling layers with a kernel size of 2 x 2 and stride as 1. If we do <a href="https://stackoverflow.com/questions/34739151/calculate-dimension-of-feature-maps-in-convolutional-neural-network">the math</a>, the resulting feature maps will take the below shapes.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/annotated_fms.png" alt="Fig 3: Annotated image and feature maps">
    <figcaption class="caption">Fig 3: Annotated image and feature maps</figcaption>
</figure>
</p>
<doc-anchor-target id="feature-vectors">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#feature-vectors">#</doc-anchor-trigger>
        <span>Feature vectors</span>
    </h2>
</doc-anchor-target>
<p>According to a research paper, this is the definition of the feature vector.</p>
<blockquote>
<p>Each feature vector of a feature sequence is generated from left to right on the feature maps by column. This means the i-th feature vector is the concatenation of the i-th columns of all the maps. The width of each column in our settings is fixed to single pixel.</p>
</blockquote>
<p>This means that for each feature map, pick a one-pixel-wide column. The resulting vector is 16 x 1 for a single feature map. Since we have 128 feature maps, the final feature vector corresponding to the first image patch will be 128 x 16. This vector will then be fed into RNN at time step t. Likewise, we can perform this until the end of our sequence. One advantage here is that RNNs can unroll into any required sequence length. Therefore, we are not limited by image width but by image height. One more advancement researchers made in the original paper was using bi-directional LSTM instead of vanilla LSTM. This allows the feature vector at the t step to communicate with the feature vector at the <em>t-k</em> and <em>t+k</em> time steps, where <em>k&gt;0</em>. Generally, this works well as it&#x27;s obvious to make predictions based on important feature vectors in both directions.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/original_architecture.png" alt="Fig 4: Model architecture from original paper">
    <figcaption class="caption">Fig 4: Model architecture from original paper</figcaption>
</figure>
</p>
<doc-anchor-target id="coding-crnn-architecture">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#coding-crnn-architecture">#</doc-anchor-trigger>
        <span>Coding CRNN architecture</span>
    </h2>
</doc-anchor-target>
<div class="flex mb-6">
    <div class="shrink-0 w-1 rounded-tl rounded-bl bg-yellow-500"></div>
    <div class="flex w-full py-4 border border-l-0 border-gray-300 rounded-tr rounded-br doc-alert bg-white dark:bg-dark-700 dark:border-dark-700" role="alert">
        <div class="flex items-center ml-4 h-7">
            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px text-yellow-500" width="22" height="22" viewBox="0 0 24 24" role="presentation">
                <g fill="currentColor"><g>
                    <path d="M22.48 15.59L14.01 1.45A2.968 2.968 0 0012.16.09c-.78-.19-1.58-.07-2.27.35-.41.25-.76.6-1.01 1.01v.01L.4 15.6c-.83 1.43-.33 3.27 1.1 4.1.45.26.95.4 1.48.4h16.95c.8-.01 1.55-.33 2.11-.9.56-.57.87-1.33.86-2.13a3.04 3.04 0 00-.42-1.48zm-1.87 2.21c-.19.19-.44.3-.69.3H2.99c-.17 0-.34-.05-.49-.13a.992.992 0 01-.37-1.35L10.6 2.48c.08-.14.2-.25.34-.33a.992.992 0 011.37.33l8.46 14.13c.09.15.13.32.13.49 0 .26-.1.51-.29.7z"></path>
                    <path d="M11.45 12.1c.55 0 1-.45 1-1v-4c0-.55-.45-1-1-1s-1 .45-1 1v4c0 .56.45 1 1 1zM11.46 14.1c-.56 0-1 .45-1 1s.45 1 1 1 1-.45 1-1-.45-1-1-1z"></path>
                </g></g>
            </svg>
        </div>
        <div class="pr-5 ml-3 w-full">
            <h5>Warning</h5>
<p>Since we are working on a relatively simple dataset, I am doing a few simplifications on top of the original model.</p>
        </div>
    </div>
</div>
<p>To make the architecture simple and intuitive, I only added two convolutions with max pooling for the CNN feature extractor, a bidirectional LSTM layer for sequence modelling and a fully connected layer. For each timestamp, the RNN + Fully connected layer will generate a probability distribution for all characters in the vocabulary. Once we have the probability distribution, we can pick a character with maximum probability. However, we must use the loss function during model training to update the parameters. Unfortunately, like language modelling, we cannot calculate negative log-likelihood (or similar) loss for this type of work, as we don&#x27;t have the desired characters for each feature vector (or timestamp). Therefore, our best bet would be to use some loss function which supports unaligned input and outputs, such as CTC loss.</p>
<doc-anchor-target id="model-usingpytorch">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#model-usingpytorch">#</doc-anchor-trigger>
        <span>Model using PyTorch</span>
    </h2>
</doc-anchor-target>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">import torch.nn as nn

class CRNN(nn.Module):
    def __init__(self, num_channels, hidden_size, num_classes):
        super(CRNN, self).__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(1, num_channels, kernel_size=(2,3), padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        self.conv2 = nn.Sequential(
            nn.Conv2d(num_channels, num_channels * 2, kernel_size=(2,3), padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )

        self.rnn = nn.LSTM(num_channels * 2 * 16, hidden_size, bidirectional=True, batch_first=True)

        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # x shape: [batch_size, channels, height, width]

        # CNN feature extraction
        conv = self.conv1(x)
        conv = self.conv2(conv)
        batch, channels, height, width = conv.size()

        conv = conv.permute(0, 3, 1, 2)  # [batch, width, channels, height]
        conv = conv.contiguous().view(batch, width, channels * height)

        rnn, _ = self.rnn(conv)

        output = self.fc(rnn)

        return output</code></pre>
</doc-codeblock></div>
<doc-anchor-target id="preprocessing-data">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#preprocessing-data">#</doc-anchor-trigger>
        <span>Preprocessing data</span>
    </h2>
</doc-anchor-target>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">import torchvision 
import torch
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence

class AddGaussianNoise(object):
    def __init__(self, mean=0., std=1., thresh=0.2):
        self.mean = mean
        self.std = std
        self.thresh = thresh
        
    def __call__(self, tensor):
        noise = torch.zeros_like(tensor)
        noise[tensor&gt;self.thresh] = 1
        noise *= torch.randn(tensor.size()) * self.std + self.mean
        return tensor + noise
    
    def __repr__(self):
        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'


class TextProcessor:
    def __init__(self, alphabet):
        self.alphabet = alphabet
        self.pad_token = &quot;[PAD]&quot;
        self.stoi = {s: i for i, s in enumerate(self.alphabet,1)}
        self.stoi[self.pad_token] = 0
        self.itos = {i: s for s, i in self.stoi.items()}
        
    def encode(self, label):
        return [self.stoi[s] for s in label]
    
    def decode(self, ids):
        return ''.join([self.itos[i] for i in ids])
    
    def __len__(self):
        return len(self.alphabet) + 1

transform_train = torchvision.transforms.Compose(
    [   
        torchvision.transforms.Grayscale(),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.RandomApply([
            torchvision.transforms.RandomAdjustSharpness(sharpness_factor=80),
            AddGaussianNoise(mean=0, std=1),
            ])
    ]
)

transform_eval = torchvision.transforms.Compose(
    [   
        torchvision.transforms.Grayscale(),
        torchvision.transforms.ToTensor()
    ]
)

class CRNNDataset(Dataset):
    def __init__(
            self, 
            height, 
            text_processor:TextProcessor, 
            transforms:torchvision.transforms,
            dataset=None
            ) -&gt; None:
        super().__init__()

        self.height = height
        self.transform = transforms
        self.dataset = dataset
        
        self.text_processor = text_processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        dset = self.dataset[idx]
        image, text = dset['image'], dset['text']
        label = torch.tensor(self.text_processor.encode(text), dtype=torch.long)
        original_width, original_height = image.size
        new_width = int(self.height * original_width / original_height)  # Calculate width to preserve aspect ratio
        image = image.resize((new_width, self.height))
        image = self.transform(image)
        return image, label
    

def collate_fn(batch):
    images, labels = zip(*batch)
    
    max_h = max(img.size(1) for img in images)
    max_w = max(img.size(2) for img in images)
    
    padded_images = []

    for img in images:
        h, w = img.size(1), img.size(2)
        padding = (0, max_w - w, 0, max_h - h)  # left, right, top, bottom
        padded_img = torch.nn.functional.pad(img, padding, mode='constant', value=0)
        padded_images.append(padded_img)
    
    images = torch.stack(padded_images, 0)
    
    target_lengths = torch.tensor([len(label) for label in labels]).long()

    labels = pad_sequence(labels, batch_first=True, padding_value=0)
    
    return images, labels, target_lengths</code></pre>
</doc-codeblock></div>
<p>For example, I have added some random transformations for training data.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/transfomerms2.png" alt="Example Transformation">
    <figcaption class="caption">Example Transformation</figcaption>
</figure>
</p>
<doc-anchor-target id="model-training">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#model-training">#</doc-anchor-trigger>
        <span>Model training</span>
    </h2>
</doc-anchor-target>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">from torch.utils.data import DataLoader
from datasets import load_dataset
import pickle
from data_processing import TextProcessor, CRNNDataset, collate_fn, transform_eval, transform_train
from utils import post_process, upload_to_hub
from multiprocessing import cpu_count
from model import CRNN
import torch.nn as nn
import torch
import wandb
import random 
from datetime import datetime
import torchvision

hf_token = 'token'
wandb_token = &quot;token&quot;

random.seed(43)
torch.manual_seed(34)


def load_data(batch_size, dataset_name):
    num_workers = cpu_count()
    dataset = load_dataset(dataset_name, num_proc=num_workers, token=hf_token)
    splits = dataset.keys()
    all_text_splits = [dataset[split]['text'] for split in splits]
    all_text = sum(all_text_splits,[])
    chars = [list(ch) for ch in all_text]
    chars_all = [c for char in chars for c in char]
    alphabet = set(chars_all)
    text_processor = TextProcessor(alphabet)
    
    if len(splits)==1:
        train_splits = test_split = 'train'
    else:
        train_splits, test_split = 'train','test'

    #pickle text processor for future use
    with open(&quot;text_process.cls&quot;,&quot;wb&quot;) as f:
        pickle.dump(text_processor, f)
    
    #upload to hfhub
    upload_to_hub(file_name='text_process.cls', token=hf_token, commit_message='Uploading text processor')
    

    dset_train = CRNNDataset(height=61, text_processor=text_processor, dataset=dataset[train_splits], transforms=transform_train)
    dset_val = CRNNDataset(height=61, text_processor=text_processor, dataset=dataset[test_split], transforms=transform_eval)

    train_dataloader = DataLoader(
        dset_train, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=num_workers
        )
    val_dataloader = DataLoader(
        dset_val, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=num_workers
        )
    
    return train_dataloader, val_dataloader, len(text_processor)


def train_crnn(model, train_loader, criterion, optimizer, device):
    model.train()
    for batch_idx, (data, labels, target_lengths) in enumerate(train_loader):
        data = data.to(device)
        labels = labels.to(device)
        target_lengths = target_lengths.to(device)

        optimizer.zero_grad()
        outputs = model(data)
        log_probs = outputs.log_softmax(dim=2)
        log_probs = log_probs.permute(1, 0, 2)
        input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.long, device=device)

        loss = criterion(log_probs, labels, input_lengths, target_lengths)
        
        loss.backward()
        optimizer.step()
    return loss.item()


def eval_crnn(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for data, labels, target_lengths in val_loader:
            data = data.to(device)
            labels = labels.to(device)
            target_lengths = target_lengths.to(device)

            outputs = model(data)
            log_probs = outputs.log_softmax(dim=2)
            log_probs = log_probs.permute(1, 0, 2)
            input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.long, device=device)

            loss = criterion(log_probs, labels, input_lengths, target_lengths)
            total_loss += loss.item()
    model.train()
    return total_loss / len(val_loader)


def predict_on_sample(model, eval_dataset:CRNNDataset, device):
    decode = eval_dataset.text_processor.decode
    idx = random.randint(0, len(eval_dataset.dataset))
    inputs = eval_dataset[idx][0]
    to_pil = torchvision.transforms.ToPILImage()
    pil_image = to_pil(inputs)
    inputs = inputs.unsqueeze(0)
    model.eval()
    
    with torch.no_grad():
        predictions = model(inputs.to(device))
    predictions = predictions.detach().cpu()
    pred_ids = torch.argmax(predictions, dim=-1).flatten().tolist()
    text = post_process(decode,pred_ids)
    return pil_image, text


def train(epochs, hidden_size, batch_size, eval_interval, device, dataset_name, lr=1e-3):
    train_dataloader, val_dataloader, num_classes = load_data(dataset_name=dataset_name, batch_size=batch_size)
    model = CRNN(num_channels=1, hidden_size=hidden_size, num_classes=num_classes)
    criterion = nn.CTCLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    model.to(device)
    train_loss = {}
    val_loss = {}
    best_eval_loss = float('inf')

    print(&quot;starting model training...&quot;)
    for epoch in range(epochs):
        loss = train_crnn(model, train_dataloader, criterion, optimizer, device)
        train_loss[epoch] = loss
        if epoch % eval_interval == 0:
            eval_loss = eval_crnn(model, val_dataloader, criterion, device)
            print(f&quot;Epoch {epoch} | Train Loss: {loss:.3f} | Val Loss: {eval_loss:.3f}&quot;)
            infer_image, predicted_text = predict_on_sample(model, eval_dataset=val_dataloader.dataset, device=device)
            log_data = {
                &quot;step&quot;: epoch,
                &quot;eval_loss&quot;: eval_loss,
                &quot;train_loss&quot;: loss,
                &quot;predictions&quot;: wandb.Image(
                    infer_image,
                    caption=f&quot;Prediction: {predicted_text}&quot;
                )
            }
            val_loss[epoch] = eval_loss
            wandb.log(log_data)

            if eval_loss &lt; best_eval_loss:
                print(&quot;Saveing best model&quot;)
                torch.save(model.state_dict(), &quot;crnn.pt&quot;)
                upload_to_hub(&quot;crnn.pt&quot;, token=hf_token, commit_message=f'Upload best model at {str(epoch)}')
                best_eval_loss = eval_loss
            


if __name__ == &quot;__main__&quot;:
    #logging related data
    wandb.login(key=wandb_token)
    today = datetime.now().strftime(&quot;%d/%m/%Y, %H:%M:%S&quot;)
    wandb_project = &quot;crnn-sinhala&quot;
    wandb_run_name = f&quot;crnn-run-{today}&quot;
    
    dataset_name = &quot;Ransaka/SSOCR-1K&quot;
    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
    epochs = 100
    batch_size = 1024
    eval_interval = int(epochs/10)
    hidden_size = 256

    #init logger
    config = dict(epochs=epochs, batch_size=batch_size,eval_intervale=eval_interval, hidden_size=hidden_size)
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)
    
    #start training
    train(
        epochs,
        hidden_size,
        batch_size,
        eval_interval,
        device,
        dataset_name
    )</code></pre>
</doc-codeblock></div>
<doc-anchor-target id="training-performance-and-testing-the-model">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#training-performance-and-testing-the-model">#</doc-anchor-trigger>
        <span>Training performance and testing the model</span>
    </h2>
</doc-anchor-target>
<p>Due to the dataset&#x27;s simplicity, the model converges without any struggle.</p>
<p><figure class="content-center">
    <img src="../../images/crnn-sinhala/loss.png" alt="Train and Validation Losses">
    <figcaption class="caption">Train and Validation Losses</figcaption>
</figure>
</p>
<doc-anchor-target id="closing-thoughts">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#closing-thoughts">#</doc-anchor-trigger>
        <span>Closing thoughts</span>
    </h2>
</doc-anchor-target>
<p>Even though the above model performed well on training data, its real-world performance is awful. This is due to several reasons: a less sophisticated dataset and a more basic model architecture (skipped batch and dropout layers). I purposely removed the in-depth explanation of CTC loss and its usefulness to make the reader more focused on the CRNN model, especially the RNN part. If you want to read more about it, I recommend <a href="https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://distill.pub/2017/ctc&amp;ved=2ahUKEwjVv4Oe76uJAxUsUGwGHUMgKckQFnoECDQQAQ&amp;usg=AOvVaw1xafucYnDRj_X48eOFZSHE">this</a> article.</p>
<p>You can access the model via <a href="https://huggingface.co/spaces/Ransaka/OCR-CRNN">this hf space</a>.</p>
<p>Some references I found useful,</p>
<ol>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
<li><a href="https://stats.stackexchange.com/questions/291820/what-is-the-definition-of-a-feature-map-aka-activation-map-in-a-convolutio">https://stats.stackexchange.com/questions/291820/what-is-the-definition-of-a-feature-map-aka-activation-map-in-a-convolutio</a></li>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></li>
<li><a href="https://stackoverflow.com/questions/34739151/calculate-dimension-of-feature-maps-in-convolutional-neural-network">https://stackoverflow.com/questions/34739151/calculate-dimension-of-feature-maps-in-convolutional-neural-network</a></li>
</ol>
<doc-anchor-target id="thank-you">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#thank-you">#</doc-anchor-trigger>
        <span><em>Thank you!!</em></span>
    </h3>
</doc-anchor-target>

                                
                                <!-- Required only on API pages -->
                                <doc-toolbar-member-filter-no-results></doc-toolbar-member-filter-no-results>
                            </div>
                            <footer class="clear-both">
                            
                                <nav class="flex mt-14">
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 h-full flex items-center break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="../../">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                            <span>
                                                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                                                <span class="block mt-1">Welcome to My Blog</span>
                                            </span>
                                        </a>
                                    </div>
                            
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="../../ransakas-blog/gmm-from-scratch/">
                                            <span>
                                                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                                                <span class="block mt-1">Gaussian Mixture Model Clearly Explained</span>
                                            </span>
                                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        </a>
                                    </div>
                                </nav>
                            </footer>
                        </main>
                
                        <div class="border-t dark:border-dark-650 pt-6 mb-8">
                            <footer class="flex flex-wrap items-center justify-between">
                                <div>
                                    <ul class="flex flex-wrap items-center text-sm">
                                    </ul>
                                </div>
                                <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2024. All rights reserved.</p></div>
                            </footer>
                        </div>
                    </div>
                
                    <!-- Rendered if sidebar right is enabled -->
                    <!-- Sidebar right skeleton-->
                    <div v-cloak class="fixed top-0 bottom-0 right-0 translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:shrink-0 lg:pt-6 lg:transform-none sm:w-1/2 lg:w-64 lg:z-0 md:w-104 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
                        <div class="pl-5">
                            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                        </div>
                    </div>
                
                    <!-- User should be able to hide sidebar right -->
                    <doc-sidebar-right v-cloak></doc-sidebar-right>
                </div>

            </div>
        </div>
    
        <doc-search-mobile></doc-search-mobile>
        <doc-back-to-top></doc-back-to-top>
    </div>


    <div id="docs-overlay-target"></div>

    <script data-cfasync="false">window.__DOCS__ = { "title": "Building CRNN model for Sinhala OCR Tasks", level: 2, icon: "file", hasPrism: true, hasMermaid: false, hasMath: false, tocDepth: 23 }</script>
</body>
</html>
