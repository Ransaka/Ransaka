[[{"l":"Welcome to My Blog","p":["Welcome to my personal blog, where I convert my experience into blog posts. You may find some of my posts useful, and I hope you enjoy reading them."]},{"i":"who-am-i","l":"Who am I?","p":["I'm Ransaka Ravihara. Computer science graduate from University of Colombo. I have morethan 4 years experience as a Data Scientist. Appart from coding and stats, I love to stay alone at nature."]},{"l":"My professional journey so far","p":["And jurney continues...","Build a dashboard using tableau to communicate business metrics to senior management in an efficient manner.","Build a disease prediction model and deploy the model using a flask.","Build a recommendation engine for Dialog's starpoints product recommendations.","Build and maintain a bayesian personalized ranking recommendation model for an online book-reading application.","Build and operationalize Fixed TV, Mobile, and HBB churn models in the cloud.","Build customer attrition prediction models for one of the leading productivity management applications in the USA.","Build multiple forecasting models for retailer store demand prediction and deploy them on AWS. Share the regional insight reports with senior management for better decision- making.","Closely engage with business units, effectively sharing technical findings with them.","Develop machine learning solutions for 15+ million customers across various business units.","Developed highly optimized data pipelines to support diverse analytical use cases.","Experiment with different data mining methods in social networks with the help of graph analytics.","Facilitated knowledge transfer sessions with team members, focusing on the latest trends in AI, particularly open-source generative AI.","Fine-tuned open-source Language Models (LLMs) using AWS and local Linux/CUDA environments to adapt them to specific domains. Build RAG pipeline to build a question answering system.","Formulate an A/B testing framework to measure the business impact of the machine learning models and the campaigns.","Lead Dialog’s mobile track and churn tribe.","Lead the data science team at Codimite Technologies.","Led and guided the data team at 3rive Technologies, ensuring efficient and successful project execution.","Leveraged advanced NLP techniques to extract valuable insights from extensive text datasets.","Manage and guide a team of 2 interns and a data analyst.","Performed customer retention analysis for one of the dating apps. Present the app’s growth rate and daily active users cohort retention rate. Analyze the app’s push notification click patterns and share the insights with app owners for better growth.","Reasearch and build scoring models for the measurement of the employee's productivity.","Skills:","Studied at University of Colombo School of Computing (UCSC)","This is where I initiated my path as a Data Scientist. In our senior year, we undertook a research project, which marked my first introduction to data science and machine learning. This experience motivated me to further pursue this field. I began diving deeper into learning Python and, as I progressed, I started actively participating on platforms like StackOverflow and Kaggle. These experiences bolstered my confidence in my skills. Fortunately, I also had the opportunity to work on some freelance projects, which marked the commencement of my professional journey as a Data Scientist.","Use explainable AI to understand model predictions. It helps to increase the customer experience and effectiveness of the campaigns.","Utilized Dialog's complex and imbalanced datasets to derive actionable insights and build machine learning models to improve customer experience and reduce churn. Launch Dialog’s first-ever fully automated churn prediction model in the cloud.","Work closely with the product team to understand the business requirements and formulate the data science roadmap.","Write clean, maintainable, and production-ready python codes and pipelines."]},{"l":"Download my resume","p":["Welcome to My Blog"]}],[{"l":"Machine Learning on Snowflake","p":["Above snowpark_df is a lazily-evaluated table; hence It won't consume much memory like pandas data frames. But we can apply any transformations aggregations and much more as we did with pandas.","After completing our analysis, we can save the transformed Dataset as a new Snowflake table using the following way.","Age distribution","Alright, now we have preprocessed Dataset. Let's start the model training phase.","Define UDFs in a pythonic way and deploy them in snowflake Topics covered in this article","If you are willing to follow along with the tutorial, you should have an Anaconda integration-enabled snowflake account. Otherwise, you must sign up for a free snowflake trial account and configure it as described here. At first glance, snowpark is a machine learning and data science framework that offers the power of SQL within Python flexibility. Sometimes, this is similar to the Apache spark framework. However, this provides a pervasive framework for our machine learning and data science projects. Before trying anything in this article, you should establish a connection between python and Snowflake. You can refer to my code repo for code samples. Let's create a database connection.","In this article, we will explore both methods above.","Let's create one more plot.","Let's find the relationship between the Age variable and the target variable.","Let's see our target distribution.","Now we can start the primary data preprocessing part. Instead of preprocessing with Pandas DataFrame, I will do this with the snowpark side. Here I'm going to use the COVID-19 Dataset, which is available in the Kaggle under CC0: Public Domain. I have already loaded this Dataset as a Snowflake table. Since it's not the primary goal of this article, I'm skipping that part. You can load the Dataset as described in this article's GitHub repo. Let's read the table.","Photo by Colin Lloyd on Unsplash","Since we are working with snowpark API, let's create this with snowpark.","Snowflake is one of the leading data platforms out there. In this article, we will explore the capabilities of its' snowpark python library.","snowpark-python functionalities for primary data preprocessing","Target distribution","There are 1,048,575 unique records and 21 columns in the Dataset. Let's do some fundamental analysis. First, let's define the target variable as follow. As per the description of the Dataset, the 1,2 and 3 values in CLASSIFICATION_FINAL the column represent the positive cases, and the rest represent the negative cases. Let's define a new column called TARGET by applying the above logic. The equivalent SQL logic will be,","Throughout this article, you will learn how to use,","Train and deploy machine learning models in Snowflake","We can use two different approaches to train and deploy models in Snowflake. We can train the model locally, upload it to a stage and load it from the stage when the UDF is called. We can define SPROC, which can train the model and save the trained model into the Snowflake stage when the SPROC is called. Here we'll need a separate UDF for the inferencing part."]},{"i":"train-the-model-locally-upload-it-to-a-stage-and-load-it-from-the-stage","l":"Train the model locally, upload it to a stage and load it from the stage","p":["Method 01","First, we have to define the function for training the model locally.","Similar to other machine learning pipelines, we need to define library dependencies.","Let's define the UDF. Inside the UDF, it should load the model from the stage and then use it for the inferencing.","Now we have successfully registered our UDF in Snowflake. You can verify it using the following way.","Let's use UDF for inferencing."]},{"i":"define-train-and-inferencing-procsudfs","l":"Define train and inferencing procs/UDFs","p":["This method will create a stored procedure for training the model and UDF for inferencing the model. You may refer to the diagram below for more insights.","Method 02","Let's define the stored procedure. At first, we will implement the Python function, and we can convert it to the Snowflake stored procedure in later steps.","Let's register the above Python function as a stored procedure.","Now we can use the procedure SPROC_TRAIN_DT_MODEL() as follows.","Feature importance","We can define the UDF as follows. This function is similar to the previous one.","Finally, registering the UDF.","Alright, it's time to get predictions for our validation dataset. Here I am doing it with Snowflake editor."]},{"l":"Conclusion","p":["While snowpark offers a comprehensive platform for our machine learning tasks, it has a few issues at the time of writing this article. As an example, PyTorch still needs to be supported by a snowpark. Also, only selected packages are available in conda; if we want to use other packages, such as catboost, we must import them manually into our environment as described here.","Thanks for reading!","Connect with me on LinkedIn."]}],[{"l":"Use SMOTE with Caution","p":["If you are a machine learning practitioner, you may face class imbalance problems more often. Class imbalance happens when there is a different class distribution in the dataset. Let's take an example. Assume we are working on a churn problem. In this specific scenario, our minor and majority classes are customer churn, and the customer stays with the current service provider. But if you explore this problem more, you will notice fewer customers for the churn category because customer churn is an infrequent event that is good for business but not for the model. As a result, if we feed this dataset into the model, it will learn the majority pattern (non-churn scenario) more accurately than the minor scenario. This is where our problem begins."]},{"i":"how-to-deal-with-class-imbalance-in-machine-learning","l":"How to deal with class imbalance in machine learning?","p":["The most obvious answer is since model interaction with the minor class during training is less, we can improve that by adding more minor classes to the model. But how? We have a few methods,","Collecting more data for the minor class — This is a theoretically easy and practically infeasible solution. Because it's hard to do this while covering the business's actual needs, as an example, we may have to change the logic to get more customers into the churn category.","Random Oversampling — We can duplicate minor classes till we get a decent class distribution. It may result in the model learning inaccurate churn scenarios. In simple words, it will over-learn little incident patterns.","Random Undersampling — We can remove the samples from the majority classes to balance the dataset. However, it will remove some signals from the dataset. Also, if our dataset is highly imbalanced (minor samples are less than 1%), we may have to remove significant majority class samples from our dataset to make it more balanced.","We can generate synthetic data — We will focus on this more deeply in this article,"]},{"l":"Generating synthetic data for rebalance the dataset","p":["ADASYN (Adaptive Synthetic Sampling) Tomek Links: This technique removes samples from the majority class that are very close to examples in the minority class. The idea is to remove easy cases from the majority class that will likely be misclassified as the minority class. Near Miss: This technique selects samples from the majority classes closest in feature space to examples in the minority class and removes them from the dataset. The idea is similar to Tomek Links, but instead of eliminating easy cases from the majority class, it removes samples most likely to be misclassified as the minority class. Let's do some experiments around SMOTE.","Are we gonna deploy these models into production?","As you can see in the plots, we have converted 220 original minor incidents into 2152, its ~ 9x increase. That's where the problem lies. Let's focus on our specific churn problem.","Change evaluation metrics","Create more features by doing error analysis for the model","First, import the dataset. Here I am using a Wine Quality, and you can access the dataset using this link. Let's load the dataset and plot the class distribution.","Generating such fake customer data can lead the model to learn patterns that do not exist in the real world.","If it's a churn or fraud detection problem, I will not deploy it on production. But the answer to the above question highly depends on the data and business problem we are working on. Generally, it's not a good idea to rely on SMOTE when data is noisy, and the problem is complex.","In most cases, we have quality issues in the dataset. As a result, there is a high chance of adding noises into datasets. Generating new data using noisy data is a bad idea.","Let's build a classifier using oversampled data and evaluate the model.","Let's dive deep into these methods.","Now it's time to experiment with other approaches for class imbalance. Below I have explained a few methods I have used to tackle imbalance problems.","Now we can use the imblearn library to perform SMOTE on our dataset. In the below code, we will do SMOTE on our dataset and plot both the original and resamples versions of the dataset.","png","The basic idea behind this is to generate more minor class samples similar to the existing ones in the minority class. But unlike repeating minor class instances multiple times, this will generate new minor class instances based on the dataset we have. For that SMOTE (Synthetic Minority Oversampling Technique) method is commonly used. But there are many alternatives for that, such as,","Use an unsupervised algorithm to detect clusters of the dataset.","Use class weights","With these potential issues, we have the below question in front."]},{"l":"Use class weights","p":["When training a model on a dataset with class imbalance, the loss function may be dominated by the majority class because it has more instances. It can cause the model to pay more attention to the majority class than the minority class. The main idea of class weights is assigning weights for each sample based on its class. It will give more weight to the minority class during training. Meaning the model will pay more attention to the minority class during training in an effort to improve its performance in that class.","In advance, class weights can be used to balance the loss function by assigning higher weights to the minority class, so that it has a greater impact on the loss function. This can help the model to better learn the characteristics of the minority class and improve its performance on it.","Mathematically, class weights are typically incorporated into the loss function by multiplying the loss for each example by the weight for its class. For example, suppose we have a dataset with two classes (0 and 1) and the following class weights:","The loss function for a binary classification model might be defined as follows:","To incorporate class weights into this loss function, we can modify it as follows:","Now, when the model is trained, the loss for each example will be multiplied by the class weight for its class. It will cause the model to pay more attention to the minority class since its samples will significantly impact the loss function.","In most major machine learning model accept sample_weight parameter. Here is an example how you can do this with XGBoost library.","One thing to notice here is that sample weights can potentially lead to overfitting if the weights are too high. It's generally a good idea to try a range of weights and see which gives the best performance on the validation set.","Alternatively, we can use the scale_pos_weight parameter in XGBoost as well. It will give you similar results.","Let's quickly plot the above model performance.","png","If you check the confusion matrix for the above two scenarios. In that case, you will notice high false positives in the oversampled scenario. We have fewer false positive predictions using class weights. It also reduced our true positives as well. So we have to tweak our approaches based on real business needs."]},{"l":"Change evaluation metrics","p":["Most libraries use Accuracy as the default evaluation metric for classification tasks. It's okay for balanced problems. But imbalanced problems will lead the model to guess the majority class without learning any potential signals.","For example, say we have 97 non-churn customers and 3 churn customers. If we are building a model used accuracy as an evaluation metric, it can achieve 97% accuracy by blindly predicting non-churn class for every 100 samples. As an easy fix, we can change the evaluation metric into something different. Precision, Recall, F1 Score, and Balanced Accuracy are a few best options for the imbalance classification task."]},{"l":"Create more features by doing error analysis for the model","p":["When your model performs poorly, we can use error analysis to find different data segments with varying performance levels. Let's take the previous churn example; we can find the model's error vs. customers' revenue bucket and identify revenue segments where the model is good and evil. Likewise, we can create an error analysis report with this information. After the error analysis report, we can determine possible new features which the model can use to distinguish the churner vs. the non-churner scenarios.","For example, if you know low revenue users are churning because of the \"XYZ\" reason, you can add that feature if it's not already in the model. Otherwise, you can perform feature engineering methods with that potential feature, such as binning the \"XYZ\" feature."]},{"l":"Use unsupervised algorithm to detect clusters of the dataset","p":["One of the powerful and popular approaches is segmentation. If we know some features that can be used for segregating data into different subgroups, we can use those features for the clustering model. After clustering, you will note various class imbalances in other groups. There are a few possible scenarios. Such as,","You may find subgroups only with one class. After verifying this customer behavior, we can further skip modeling for this particular subgroup. It will reduce the imbalance of the whole dataset.","You may find a somewhat balanced distribution that is easy to model compared to the previous dataset.","Or, you may find subgroups with a highly imbalanced class distribution. But this is not lousy compared to the original distribution. The reason for this is this imbalance occurs in similar data points and can be a strong signal even though the distribution is imbalanced. For example, if we are working on a disease prediction model, it's a good idea to group people into subgroups based on age. If the general class imbalance is 2%, grouping on age will produce different class distributions for different subgroups. In the higher age group, this will be more balanced. In the middle and young age groups, this will be highly imbalanced. Since we are modeling each segment separately, the model can generalize to that particular segment well."]},{"l":"Conclusion","p":["This article aims to show alternatives to treat class imbalance other than synthetic data generation. It's worth noting that some methods heavily depend on the data, problem type, and domain you are working on. It's always a good idea to experiment with a few different approaches and pick the best one for your problem.","Please find the citation for the above dataset.","Lemaitre, G., Nogueira, F., Aridas, C. K., & Oliveira, D. V. R. (2016). Imbalanced dataset for benchmarking [Data set]. Zenodo. https://doi.org/10.5281/zenodo.61452_","Thanks for reading."]}],[{"l":"Projects"},{"l":"Daily Llama","p":["Retrieval augmented generation framework based on news contents.","This project presents a retrieval-augmented generation framework tailored for news content. By employing the FAISS index for efficient similarity search, relevant documents are retrieved. Leveraging the LLAMA-2 model, these retrieved documents serve as a foundation for generating comprehensive and contextually accurate answers. Github"]},{"l":"First Ever Sinhala Sentence Transformer Model","p":["Indepedent research project to create the first ever Sinhala Sentence Transformer Model. I was able to create a model that can be used to encode Sinhala sentences into vectors. This model can be used for various NLP tasks such as sentence similarity, sentence classification, etc. Further, I was able to create a BERT model that can be used for Sinhala masked language modeling. For more information, please refer to the HuggingFace space below. Sinhala Embeddings"]},{"l":"Sinhala Zero-Shot Text Classification","p":["Another research project I did to create a zero-shot text classification model for Sinhala. As part of this project, I have trained 3 BERT models with different architectures. Working demo is available in the HuggingFace space below. Sinhala Zero Shot Classification"]}],[{"l":"Reach out to me"},{"l":"Email","p":["ran@gmail.com"]}]]