[[{"l":"Welcome to My Blog","p":["Welcome to my personal blog, where I convert my experience into blog posts. You may find some of my posts useful, and I hope you enjoy reading them."]},{"l":"Who I am","p":["I'm Ransaka Ravihara, and I graduated in Computer Science from the University of Colombo School of Computing. I have over four years of experience as a Data Scientist. Aside from working with data and code, I enjoy spending time alone in nature."]},{"l":"My professional journey so far","p":["And jurney continues...","Build a dashboard using tableau to communicate business metrics to senior management in an efficient manner.","Build a disease prediction model and deploy the model using a flask.","Build a recommendation engine for Dialog's starpoints product recommendations.","Build and maintain a bayesian personalized ranking recommendation model for an online book-reading application.","Build and operationalize Fixed TV, Mobile, and HBB churn models in the cloud.","Build customer attrition prediction models for one of the leading productivity management applications in the USA.","Build multiple forecasting models for retailer store demand prediction and deploy them on AWS. Share the regional insight reports with senior management for better decision- making.","Closely engage with business units, effectively sharing technical findings with them.","Develop machine learning solutions for 15+ million customers across various business units.","Developed highly optimized data pipelines to support diverse analytical use cases.","Experiment with different data mining methods in social networks with the help of graph analytics.","Facilitated knowledge transfer sessions with team members, focusing on the latest trends in AI, particularly open-source generative AI.","Fine-tuned open-source Language Models (LLMs) using AWS and local Linux/CUDA environments to adapt them to specific domains. Build RAG pipeline to build a question answering system.","Formulate an A/B testing framework to measure the business impact of the machine learning models and the campaigns.","Lead Dialog’s mobile track and churn tribe.","Lead the data science team at Codimite Technologies.","Led and guided the data team at 3rive Technologies, ensuring efficient and successful project execution.","Leveraged advanced NLP techniques to extract valuable insights from extensive text datasets.","Manage and guide a team of 2 interns and a data analyst.","Performed customer retention analysis for one of the dating apps. Present the app’s growth rate and daily active users cohort retention rate. Analyze the app’s push notification click patterns and share the insights with app owners for better growth.","Reasearch and build scoring models for the measurement of the employee's productivity.","Skills:","Studied at University of Colombo School of Computing (UCSC)","This is where I initiated my path as a Data Scientist. In our senior year, we undertook a research project, which marked my first introduction to data science and machine learning. This experience motivated me to further pursue this field. I began diving deeper into learning Python and, as I progressed, I started actively participating on platforms like StackOverflow and Kaggle. These experiences bolstered my confidence in my skills. Fortunately, I also had the opportunity to work on some freelance projects, which marked the commencement of my professional journey as a Data Scientist.","Use explainable AI to understand model predictions. It helps to increase the customer experience and effectiveness of the campaigns.","Use langchain to build interactive chatbots with memory.","Use LoRA fine-tuning to adapt LLMs into specific domains.","Utilized Dialog's complex and imbalanced datasets to derive actionable insights and build machine learning models to improve customer experience and reduce churn. Launch Dialog’s first-ever fully automated churn prediction model in the cloud.","Work closely with the product team to understand the business requirements and formulate the data science roadmap.","Write clean, maintainable, and production-ready python codes and pipelines."]}],[{"i":"building-crnn-model-for-sinhala-ocrtasks","l":"Building CRNN model for Sinhala OCR Tasks"},{"i":"step-by-step-guide-to-training-own-ocrmodel","l":"Step-by-step guide to training own OCR model","p":["Welcome"]},{"l":"Opening thoughts","p":["If you're reading this article, you likely have different interests than the majority. It's rare to find someone wanting to build a CRNN model in this era of LLM hype, which makes this quite refreshing. Next, I want to share my motivation behind this article. However, if you feel that's unnecessary, you can directly jump into this section. So, my motivation behind this article(possibly an article series) is making people want to play around with RNNs. RNNs are the pioneers in this sequence modelling before transformers come into play. Most concepts we are now trying out with transformers have evolved from RNNs. Therefore, I believe these concepts with RNNs make much more sense than those with more complex and heavy-weight transformer architecture. As per this thought process, I decided to go ahead with it. While I learn these, I am using these articles to digest these concepts better. Alright, enough talking. Let's jump into the actual article."]},{"l":"Understanding CRNN","p":["CRNN (Convolutional Recurrent Neural Network) was first proposed in the paper An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. As the name implies, the architecture comprises a convolutional neural network with an RNN model. But how? Let me explain. First, forget about these terms: RNNs, CNNs, etc. How would you do that if you were supposed to read the text in the image below?","Sample Image via MJSynth","It's pretty simple, right? All you need to know is to scan the image with your eyes. Once you understand the first character, you shift right (or left, depending on the language). You continue this process until you reach the end of the image. This is exactly what CRNNs do under the hood. The only difference is that you've trained to do this throughout your life, unlike these models, which only have a few days to achieve the same results. ¯/(ツ)/¯","But there is a tiny difference. We humans can understand a character at first glance.","Fig 1","On the other hand, machines are quite incapable of this as they are limited by capturing only spatial representation, the amount of data they can access at certain points and, more importantly, the nature of data we put in. Intuitively, if we need to build a system capable of predicting corresponding text in certain regions, we should have a properly annotated dataset. One possibility would be to create patches of images and assign characters to each, like this.","Fig 2 Map different image patches to a label","In practice, creating this kind of dataset requires some serious manpower and careful annotations. But we can do the trick; what if we allow a CNN model to self-create these patches and then label each patch based on its spatial representation? That might work, isn't it? But again, an obstacle: If you check the above image, you will notice that you need both the first and second patches to predict S. The same goes with E and C above. Therefore, we will predict the current label based on information from previous patches and the current patch itself. In other words, it's sequence modelling. And that's where RNN comes in here. So far in the explanation, you have probably noted how we transitioned this OCR task into CNN and RNN. More intuitively, that is why they called this CRNN. It has tackled the OCR problem using a combination of CNN and RNN. CNN to learn spatial features in the image, then pass into RNN to detect the relationship between patches(feature maps) and produce the final output. I'm using this dataset in the later part of this article. It contains synthetic images suitable for OCR tasks. You can access the generation script in this code repository. Furthermore, all codes related to this article can be accessed from the same GitHub repository.","SSOCR Dataset"]},{"i":"more-detailed-explanation-about-the-model-architecture","l":"More detailed explanation about the model architecture.","p":["As initially discussed, it has two components: CNN and RNN. However, the original paper stated a third component, the transcription layer. That's where we post-process predicted output and generate final predictions. Furthermore, unlike traditional CNN, we don't need to use a fully connected layer as the final layer, as final feature maps are transformed into feature vectors and then passed to RNN. This results in many-to-one mapping between feature vectors and corresponding labels. In the dataset, it's just an image, and text appears (See Fig 2). Hence, the model is supposed to align the text with a sequence of feature vectors by itself."]},{"i":"feature-vectors-feature-maps-and-imagepatches","l":"Feature vectors, feature maps and image patches","p":["Perhaps you needed clarification on these three, as I often used them in the explanation. Here's the explanation. Image patches are horizontal or vertical (or a combination of both) image segments, as shown in Fig. That has nothing to do with this CRNN model. I only used it in the article to get the reader used to the concept of CRNN. However, feature maps and feature vectors deserve more explanation because they are major components of CRNN architecture."]},{"l":"Feature maps","p":["Feature maps are the product of CNN. When the image passes through convolution and pooling layers, it transforms into several feature maps. Each convolution layer applies a different filter, and when we perform this for multiple layers, we have a very abstract view of the input image. A pooling layer is also applied between convolution layers to retain the most usable feature. This will reduce the resulting feature map dimension. Ultimately, the resulting feature maps can be considered feature-rich and efficient representations of the input image. Due to dimensionality reduction, a small area in a resulting feature map corresponds to a larger area in the input image. We call that receptive field. Nevertheless, to understand feature maps in the context of CRNN, all you need to know is that feature maps have very specialized representations of input features, which we transform as feature vectors.","Let's assume we have a 61 x 157 image. Our model comprises two convolution and max pooling layers with a kernel size of 2 x 2 and stride as 1. If we do the math, the resulting feature maps will take the below shapes.","Fig 3: Annotated image and feature maps"]},{"l":"Feature vectors","p":["According to a research paper, this is the definition of the feature vector.","Each feature vector of a feature sequence is generated from left to right on the feature maps by column. This means the i-th feature vector is the concatenation of the i-th columns of all the maps. The width of each column in our settings is fixed to single pixel.","This means that for each feature map, pick a one-pixel-wide column. The resulting vector is 16 x 1 for a single feature map. Since we have 128 feature maps, the final feature vector corresponding to the first image patch will be 128 x 16. This vector will then be fed into RNN at time step t. Likewise, we can perform this until the end of our sequence. One advantage here is that RNNs can unroll into any required sequence length. Therefore, we are not limited by image width but by image height. One more advancement researchers made in the original paper was using bi-directional LSTM instead of vanilla LSTM. This allows the feature vector at the t step to communicate with the feature vector at the t-k and t+k time steps, where k>0. Generally, this works well as it's obvious to make predictions based on important feature vectors in both directions.","Fig 4: Model architecture from original paper"]},{"l":"Coding CRNN architecture","p":["Since we are working on a relatively simple dataset, I am doing a few simplifications on top of the original model.","To make the architecture simple and intuitive, I only added two convolutions with max pooling for the CNN feature extractor, a bidirectional LSTM layer for sequence modelling and a fully connected layer. For each timestamp, the RNN + Fully connected layer will generate a probability distribution for all characters in the vocabulary. Once we have the probability distribution, we can pick a character with maximum probability. However, we must use the loss function during model training to update the parameters. Unfortunately, like language modelling, we cannot calculate negative log-likelihood (or similar) loss for this type of work, as we don't have the desired characters for each feature vector (or timestamp). Therefore, our best bet would be to use some loss function which supports unaligned input and outputs, such as CTC loss."]},{"i":"model-usingpytorch","l":"Model using PyTorch"},{"l":"Preprocessing data","p":["For example, I have added some random transformations for training data.","Example Transformation"]},{"l":"Model training"},{"l":"Training performance and testing the model","p":["Due to the dataset's simplicity, the model converges without any struggle.","Train and Validation Losses"]},{"l":"Closing thoughts","p":["Even though the above model performed well on training data, its real-world performance is awful. This is due to several reasons: a less sophisticated dataset and a more basic model architecture (skipped batch and dropout layers). I purposely removed the in-depth explanation of CTC loss and its usefulness to make the reader more focused on the CRNN model, especially the RNN part. If you want to read more about it, I recommend this article.","You can access the model via this hf space.","Some references I found useful,","https://colah.github.io/posts/2015-08-Understanding-LSTMs/","https://stats.stackexchange.com/questions/291820/what-is-the-definition-of-a-feature-map-aka-activation-map-in-a-convolutio","https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/","https://stackoverflow.com/questions/34739151/calculate-dimension-of-feature-maps-in-convolutional-neural-network"]},{"i":"thank-you","l":"Thank you!!"}],[{"l":"Gaussian Mixture Model Clearly Explained"},{"l":"The only guide you need to learn everything about GMM","p":["When we talk about Gaussian Mixture Model (later, this will be denoted as GMM in this article), it's essential to know how the KMeans algorithm works. Because GMM is quite similar to the KMeans, more likely it's a probabilistic version of KMeans. This probabilistic feature allows GMM to be applied to many complex problems that KMeans can't fit into.","In summary, KMeans have below limitations,","It assumed that the clusters were spherical and equally sized, which is not valid in most real-world scenarios.","It's a hard clustering method. Meaning each data point is assigned to a single cluster.","Due to these limitations, we should know alternatives for KMeans when working on our machine learning projects. In this article, we will explore one of the best alternatives for KMeans clustering, called the Gaussian Mixture Model.","Throughout this article, we will be covering the below points.","How Gaussian Mixture Model (GMM) algorithm works — in plain English.","Mathematics behind GMM.","Implement GMM using Python from scratch."]},{"i":"how-gaussian-mixture-model-gmm-algorithm-works--in-plain-english","l":"How Gaussian Mixture Model (GMM) algorithm works — in plain English","p":["How Gaussian Mixture Model (GMM) algorithm works — in plain English As I have mentioned earlier, we can call GMM probabilistic KMeans because the starting point and training process of the KMeans and GMM are the same. However, KMeans uses a distance-based approach, and GMM uses a probabilistic approach. There is one primary assumption in GMM: the dataset consists of multiple Gaussians, in other words, a mixture of the gaussian.","png","The above kind of distribution is often called multi-model distribution. Each peak represents the different gaussian distribution or the cluster in our dataset. But the question is,"]},{"i":"how-do-we-estimate-these-distributions","l":"how do we estimate these distributions?","p":["Before answering this question, let's create some gaussian distribution first. Please note here I am generating multivariate normal distribution; it's a higher dimensional extension of the univariate normal distribution.","Let's define the mean and covariance of our data points. Using mean and covariance, we can generate the distribution as follows.","png","As you can see here, we generated random gaussian distribution using mean and covariance matrices. What about reversing this process? That's what exactly GMM is doing. But how?","Because, in the beginning, we didn’t have any insights about clusters nor their associated mean and covariance matrices","Well, It happens according to the below steps,","Decide the number of clusters (to decide this, we can use domain knowledge or other methods such as BIC/AIC) for the given dataset. Assume that we have 1000 data points, and we set the number of groups as 2.","Initiate mean, covariance, and weight parameter per cluster. (we will explore more about this in a later section)","Use the Expectation Maximization algorithm to do the following,","Expectation Step (E step): Calculate the probability of each data point belonging to each data point, then evaluate the likelihood function using the current estimate for the parameters","Maximization step (M step): Update the previous mean, covariance, and weight parameters to maximize the expected likelihood found in the E step","Repeat these steps until the model converges.","With this information, I am concluding the no-math explanation of the GMM algorithm."]},{"l":"Mathematics behind GMM","p":["Additional Fact:","covariance (Σ): initialize randomly","eq2.png","equatios3.png","For each data point \uD835\uDC65\uD835\uDC56: Calculate the probability that the data point belongs to cluster (\uD835\uDC50) using the below equation. k is the number of distributions we are supposed to find.","image.png","images/eq1.png","In this step, the algorithm uses the responsibilities of the Gaussian distributions (computed in the E-step) to update the estimates of the model's parameters.","Let's demonstrate the EM algorithm in the sense of GMM.","Let's take a simple example. Assume we have 100 data points and need to cluster them into two groups. We can write r_ic(i=20,c=1) as follows. Where i represents the data point's index, and c represents the index of the cluster we are considering.","Lots of ugly and complex equations, right? :)","mean (μ): initialize randomly.","Please note at the beginning, \uD835\uDF0B_\uD835\uDC50 initialized to equal for each cluster c = 1,2,3,..,k. In our case, \uD835\uDF0B_1 = \uD835\uDF0B_2 = 1/2.","So on and so forth, this process will repeat until algorithm convergence, typically achieved when the model parameters do not change significantly from one iteration to the next.","Step 01: Initialize mean, covariance and weight parameters","Step 02: Expectation Step (E step)","Step 03: Maximization Step (M step)","The core of GMM lies within Expectation Maximization(EM) algorithm described in the previous section.","The E-step computes these probabilities using the current estimates of the model's parameters. These probabilities are typically referred to as the \"responsibilities\" of the Gaussian distributions. They are represented by the variables r_ic, where i is the index of the data point, and c is the index of the Gaussian distribution. The responsibility measures how much the c-th Gaussian distribution is responsible for generating the i-th data point. Conditional probability is used here, more specifically, Bayes theorem.","The M-step updates the estimates of the parameters as follows:","The result of the E-step is a set of responsibilities for each data point and each Gaussian distribution in the mixture model. These responsibilities are used in the M-step to update the estimates of the model's parameters.","Then update the Σc using the 6th equation.","This updated estimate is used in the next E-step to compute new responsibilities for the data points.","Update the μc using equation number 5 above.","Update the πc (mixing coefficients) using equation 4 above.","weight (mixing coefficients) (π): fraction per class refers to the likelihood that a particular data point belongs to each class. In the beginning, this will be equal for all clusters. Assume that we fit a GMM with three components. In this case weight parameter might be set to 1/3 for each component, resulting in a probability distribution of (1/3, 1/3, 1/3).","Where \uD835\uDF0B_\uD835\uDC50 is the mixing coefficient (sometimes called weight) for the Gaussian distribution c, which was initialized in the previous stage, and \uD835\uDC41(\uD835\uDC99 | \uD835\uDF41,\uD835\uDEBA) describes the probability density function (PDF) of a Gaussian distribution with mean \uD835\uDF07 and covariance Σ with respect to data point x; We can denote it as below.","πc can be considered equivalent to the fraction of points allocated to \uD835\uDC50 because numerator Σ_\uD835\uDC56 *\uD835\uDC5F_\uD835\uDC56\uD835\uDC50 represents the likelihood of the data point belonging to the gaussian c. If we assume we have 3 clusters and \uD835\uDC56-th data point belongs to cluster 1, we can write the related vector as [0.97,0.02,0.01]. If we sum these vectors for each data point, the result vector is approximately equal to the number of data points per cluster."]},{"i":"lets-summarize-the-above-facts-into-one-simple-diagram","l":"Let’s summarize the above facts into one simple diagram,","p":["summary.png","Don't worry; when it comes to coding, it will be one line per each equation. Let's start to implement GMM from scratch using Python."]},{"i":"implement-gmm-using-python-from-scratch","l":"Implement GMM using Python from scratch.","p":["After this function, we covered the first two equations we discussed in E Step. Here we have generated the gaussian distribution for the current model parameter means and variances. We accomplished that by using the scipy's stat module. After, we used the pdf method to calculate the likelihood of belonging to each data point for each cluster.","Alright, let's see how our handcrafted GMM performs.","And plot the generated data as follows. Please note that instead of plotting the data itself, I have plotted the probability density of each sample.","animated_GMM new.gif","But in the actual use cases, you will use the scikit-learn version of the GMM more often. There you can find additional parameters, such as","First thing first, let's create a fake dataset. In this section, I will implement GMM for the 1-D dataset.","In the above diagrams, red dashed lines represent the original distribution, while other graphs represent the learned distributions. After the 30th iteration, we can see that our model performed well on this toy dataset.","init_params: The method used to initialize the weights","Let's build each step described in the previous section,","Let's implement the training loop.","model traininng.png","png","Step 01: Initialize mean, covariance, and weights","Step 02: Expectation Step (E step)","Step 03: Maximization Step (M step)","tol: defining the model’s stop criteria. EM iterations will stop when the lower bound average gain is below the tol parameter.","When we start the model training, we will do E and M steps according to the n_steps parameter we set.","You may refer to the documentation here"]}],[{"l":"Machine Learning on Snowflake","p":["Above snowpark_df is a lazily-evaluated table; hence It won't consume much memory like pandas data frames. But we can apply any transformations aggregations and much more as we did with pandas.","After completing our analysis, we can save the transformed Dataset as a new Snowflake table using the following way.","Age distribution","Alright, now we have preprocessed Dataset. Let's start the model training phase.","Define UDFs in a pythonic way and deploy them in snowflake Topics covered in this article","If you are willing to follow along with the tutorial, you should have an Anaconda integration-enabled snowflake account. Otherwise, you must sign up for a free snowflake trial account and configure it as described here. At first glance, snowpark is a machine learning and data science framework that offers the power of SQL within Python flexibility. Sometimes, this is similar to the Apache spark framework. However, this provides a pervasive framework for our machine learning and data science projects. Before trying anything in this article, you should establish a connection between python and Snowflake. You can refer to my code repo for code samples. Let's create a database connection.","In this article, we will explore both methods above.","Let's create one more plot.","Let's find the relationship between the Age variable and the target variable.","Let's see our target distribution.","Now we can start the primary data preprocessing part. Instead of preprocessing with Pandas DataFrame, I will do this with the snowpark side. Here I'm going to use the COVID-19 Dataset, which is available in the Kaggle under CC0: Public Domain. I have already loaded this Dataset as a Snowflake table. Since it's not the primary goal of this article, I'm skipping that part. You can load the Dataset as described in this article's GitHub repo. Let's read the table.","Photo by Colin Lloyd on Unsplash","Since we are working with snowpark API, let's create this with snowpark.","Snowflake is one of the leading data platforms out there. In this article, we will explore the capabilities of its' snowpark python library.","snowpark-python functionalities for primary data preprocessing","Target distribution","There are 1,048,575 unique records and 21 columns in the Dataset. Let's do some fundamental analysis. First, let's define the target variable as follow. As per the description of the Dataset, the 1,2 and 3 values in CLASSIFICATION_FINAL the column represent the positive cases, and the rest represent the negative cases. Let's define a new column called TARGET by applying the above logic. The equivalent SQL logic will be,","Throughout this article, you will learn how to use,","Train and deploy machine learning models in Snowflake","We can use two different approaches to train and deploy models in Snowflake. We can train the model locally, upload it to a stage and load it from the stage when the UDF is called. We can define SPROC, which can train the model and save the trained model into the Snowflake stage when the SPROC is called. Here we'll need a separate UDF for the inferencing part."]},{"i":"train-the-model-locally-upload-it-to-a-stage-and-load-it-from-the-stage","l":"Train the model locally, upload it to a stage and load it from the stage","p":["Method 01","First, we have to define the function for training the model locally.","Similar to other machine learning pipelines, we need to define library dependencies.","Let's define the UDF. Inside the UDF, it should load the model from the stage and then use it for the inferencing.","Now we have successfully registered our UDF in Snowflake. You can verify it using the following way.","Let's use UDF for inferencing."]},{"i":"define-train-and-inferencing-procsudfs","l":"Define train and inferencing procs/UDFs","p":["This method will create a stored procedure for training the model and UDF for inferencing the model. You may refer to the diagram below for more insights.","Method 02","Let's define the stored procedure. At first, we will implement the Python function, and we can convert it to the Snowflake stored procedure in later steps.","Let's register the above Python function as a stored procedure.","Now we can use the procedure SPROC_TRAIN_DT_MODEL() as follows.","Feature importance","We can define the UDF as follows. This function is similar to the previous one.","Finally, registering the UDF.","Alright, it's time to get predictions for our validation dataset. Here I am doing it with Snowflake editor."]},{"l":"Conclusion","p":["While snowpark offers a comprehensive platform for our machine learning tasks, it has a few issues at the time of writing this article. As an example, PyTorch still needs to be supported by a snowpark. Also, only selected packages are available in conda; if we want to use other packages, such as catboost, we must import them manually into our environment as described here.","Thanks for reading!","Connect with me on LinkedIn."]}],[{"l":"Tokenization 101"},{"l":"Necessary Imports and installs","p":["Tokenization stands as a foundational principle in natural language processing (NLP), encompassing the division of a text into smaller entities referred to as tokens. These tokens may take the form of words, characters, or subwords, contingent on the chosen tokenization approach. Upon completion of the tokenization process, each token is associated with a unique identifier. To sum up, the tokenization process functions as a means of transforming textual data into a numerical format."]},{"l":"Why We Need Tokenization","p":["Text Understanding: Tokenization enables computers to understand and process human language more effectively by breaking down complex sentences into simpler units.","Feature Extraction: In NLP tasks, tokens serve as features for machine learning models, allowing them to learn patterns and relationships within the data. (Model can't process raw text)"]},{"l":"Different Types of Tokenization"},{"l":"Word-based Tokenization","p":["Word-based tokenization involves breaking down text into individual words. Each word becomes a separate token.","For example:"]},{"l":"Character-based Tokenization","p":["Character-based tokenization breaks text into individual characters. Each character becomes a separate token.","For example:"]},{"l":"Subword-based Tokenization","p":["Subword-based tokenization involves breaking down text into smaller units that may represent meaningful subwords or partial words. This method is particularly useful for handling rare or out-of-vocabulary words.","For example:","Frequently occured words should not be split into smaller subwords, but rare words should be decomposed into subwords."]},{"l":"Summary","p":["Screenshot 2023-11-26 at 14.29.01.png"]},{"i":"next-video-tokenization-with-hugging-face-transformers-library","l":"Next Video: Tokenization with Hugging Face Transformers Library","p":["In the upcoming video of this series, we will explore the practical implementation of tokenization using the Hugging Face Transformers Library. This powerful library provides pre-trained models and tokenizers that can be seamlessly integrated into your NLP projects.","Stay tuned!"]}],[{"l":"Use SMOTE with Caution","p":["If you are a machine learning practitioner, you may face class imbalance problems more often. Class imbalance happens when there is a different class distribution in the dataset. Let's take an example. Assume we are working on a churn problem. In this specific scenario, our minor and majority classes are customer churn, and the customer stays with the current service provider. But if you explore this problem more, you will notice fewer customers for the churn category because customer churn is an infrequent event that is good for business but not for the model. As a result, if we feed this dataset into the model, it will learn the majority pattern (non-churn scenario) more accurately than the minor scenario. This is where our problem begins."]},{"i":"how-to-deal-with-class-imbalance-in-machine-learning","l":"How to deal with class imbalance in machine learning?","p":["The most obvious answer is since model interaction with the minor class during training is less, we can improve that by adding more minor classes to the model. But how? We have a few methods,","Collecting more data for the minor class — This is a theoretically easy and practically infeasible solution. Because it's hard to do this while covering the business's actual needs, as an example, we may have to change the logic to get more customers into the churn category.","Random Oversampling — We can duplicate minor classes till we get a decent class distribution. It may result in the model learning inaccurate churn scenarios. In simple words, it will over-learn little incident patterns.","Random Undersampling — We can remove the samples from the majority classes to balance the dataset. However, it will remove some signals from the dataset. Also, if our dataset is highly imbalanced (minor samples are less than 1%), we may have to remove significant majority class samples from our dataset to make it more balanced.","We can generate synthetic data — We will focus on this more deeply in this article,"]},{"l":"Generating synthetic data for rebalance the dataset","p":["ADASYN (Adaptive Synthetic Sampling) Tomek Links: This technique removes samples from the majority class that are very close to examples in the minority class. The idea is to remove easy cases from the majority class that will likely be misclassified as the minority class. Near Miss: This technique selects samples from the majority classes closest in feature space to examples in the minority class and removes them from the dataset. The idea is similar to Tomek Links, but instead of eliminating easy cases from the majority class, it removes samples most likely to be misclassified as the minority class. Let's do some experiments around SMOTE.","Are we gonna deploy these models into production?","As you can see in the plots, we have converted 220 original minor incidents into 2152, its ~ 9x increase. That's where the problem lies. Let's focus on our specific churn problem.","Change evaluation metrics","Create more features by doing error analysis for the model","First, import the dataset. Here I am using a Wine Quality, and you can access the dataset using this link. Let's load the dataset and plot the class distribution.","Generating such fake customer data can lead the model to learn patterns that do not exist in the real world.","If it's a churn or fraud detection problem, I will not deploy it on production. But the answer to the above question highly depends on the data and business problem we are working on. Generally, it's not a good idea to rely on SMOTE when data is noisy, and the problem is complex.","In most cases, we have quality issues in the dataset. As a result, there is a high chance of adding noises into datasets. Generating new data using noisy data is a bad idea.","Let's build a classifier using oversampled data and evaluate the model.","Let's dive deep into these methods.","Now it's time to experiment with other approaches for class imbalance. Below I have explained a few methods I have used to tackle imbalance problems.","Now we can use the imblearn library to perform SMOTE on our dataset. In the below code, we will do SMOTE on our dataset and plot both the original and resamples versions of the dataset.","png","The basic idea behind this is to generate more minor class samples similar to the existing ones in the minority class. But unlike repeating minor class instances multiple times, this will generate new minor class instances based on the dataset we have. For that SMOTE (Synthetic Minority Oversampling Technique) method is commonly used. But there are many alternatives for that, such as,","Use an unsupervised algorithm to detect clusters of the dataset.","Use class weights","With these potential issues, we have the below question in front."]},{"l":"Use class weights","p":["When training a model on a dataset with class imbalance, the loss function may be dominated by the majority class because it has more instances. It can cause the model to pay more attention to the majority class than the minority class. The main idea of class weights is assigning weights for each sample based on its class. It will give more weight to the minority class during training. Meaning the model will pay more attention to the minority class during training in an effort to improve its performance in that class.","In advance, class weights can be used to balance the loss function by assigning higher weights to the minority class, so that it has a greater impact on the loss function. This can help the model to better learn the characteristics of the minority class and improve its performance on it.","Mathematically, class weights are typically incorporated into the loss function by multiplying the loss for each example by the weight for its class. For example, suppose we have a dataset with two classes (0 and 1) and the following class weights:","The loss function for a binary classification model might be defined as follows:","To incorporate class weights into this loss function, we can modify it as follows:","Now, when the model is trained, the loss for each example will be multiplied by the class weight for its class. It will cause the model to pay more attention to the minority class since its samples will significantly impact the loss function.","In most major machine learning model accept sample_weight parameter. Here is an example how you can do this with XGBoost library.","One thing to notice here is that sample weights can potentially lead to overfitting if the weights are too high. It's generally a good idea to try a range of weights and see which gives the best performance on the validation set.","Alternatively, we can use the scale_pos_weight parameter in XGBoost as well. It will give you similar results.","Let's quickly plot the above model performance.","png","If you check the confusion matrix for the above two scenarios. In that case, you will notice high false positives in the oversampled scenario. We have fewer false positive predictions using class weights. It also reduced our true positives as well. So we have to tweak our approaches based on real business needs."]},{"l":"Change evaluation metrics","p":["Most libraries use Accuracy as the default evaluation metric for classification tasks. It's okay for balanced problems. But imbalanced problems will lead the model to guess the majority class without learning any potential signals.","For example, say we have 97 non-churn customers and 3 churn customers. If we are building a model used accuracy as an evaluation metric, it can achieve 97% accuracy by blindly predicting non-churn class for every 100 samples. As an easy fix, we can change the evaluation metric into something different. Precision, Recall, F1 Score, and Balanced Accuracy are a few best options for the imbalance classification task."]},{"l":"Create more features by doing error analysis for the model","p":["When your model performs poorly, we can use error analysis to find different data segments with varying performance levels. Let's take the previous churn example; we can find the model's error vs. customers' revenue bucket and identify revenue segments where the model is good and evil. Likewise, we can create an error analysis report with this information. After the error analysis report, we can determine possible new features which the model can use to distinguish the churner vs. the non-churner scenarios.","For example, if you know low revenue users are churning because of the \"XYZ\" reason, you can add that feature if it's not already in the model. Otherwise, you can perform feature engineering methods with that potential feature, such as binning the \"XYZ\" feature."]},{"l":"Use unsupervised algorithm to detect clusters of the dataset","p":["One of the powerful and popular approaches is segmentation. If we know some features that can be used for segregating data into different subgroups, we can use those features for the clustering model. After clustering, you will note various class imbalances in other groups. There are a few possible scenarios. Such as,","You may find subgroups only with one class. After verifying this customer behavior, we can further skip modeling for this particular subgroup. It will reduce the imbalance of the whole dataset.","You may find a somewhat balanced distribution that is easy to model compared to the previous dataset.","Or, you may find subgroups with a highly imbalanced class distribution. But this is not lousy compared to the original distribution. The reason for this is this imbalance occurs in similar data points and can be a strong signal even though the distribution is imbalanced. For example, if we are working on a disease prediction model, it's a good idea to group people into subgroups based on age. If the general class imbalance is 2%, grouping on age will produce different class distributions for different subgroups. In the higher age group, this will be more balanced. In the middle and young age groups, this will be highly imbalanced. Since we are modeling each segment separately, the model can generalize to that particular segment well."]},{"l":"Conclusion","p":["This article aims to show alternatives to treat class imbalance other than synthetic data generation. It's worth noting that some methods heavily depend on the data, problem type, and domain you are working on. It's always a good idea to experiment with a few different approaches and pick the best one for your problem.","Please find the citation for the above dataset.","Lemaitre, G., Nogueira, F., Aridas, C. K., & Oliveira, D. V. R. (2016). Imbalanced dataset for benchmarking [Data set]. Zenodo. https://doi.org/10.5281/zenodo.61452_","Thanks for reading."]}],[{"i":"#","p":["How to use llama 3 for building AI Agents"]},{"l":"Using Llama 3 for Building AI Agents","p":["Comprehensive guide to building AI Agents with Llama 3 function calling capabilities.","Image by Author via Canva"]},{"l":"Introduction","p":["Imagine you want to buy something. You visit an e-commerce website and use the search option to find what you want. Maybe you have multiple items to buy, so the process isn’t very efficient. Now consider this scenario: open an application, describe what you want in plain English, and press enter. You don't have to worry about searching and price comparisons because the application handles it automatically for you. Pretty cool, right? That’s exactly what we’ll build in this tutorial.","Let’s look at some examples first.","User asking multiple products at once","User is asking for the most cost-effective purchase he/she can make.","Alright, let’s bring life to this application. We’re going to use Meta’s Llama 3 model with function calling capability. However, this can also be accomplished using the 3.1 models. According to Meta’s announcement, the 3.1 models can use tools and functions more effectively.","These are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities","I will use Groq Cloud, specifically their model for this article. The initial workflow of this application should consist of an embedding model, a retriever, and two major tools for handling user purchase interests and cost-related concerns. In summary, we need something similar to what we've described in the diagram below.","Architecture Diagram","Now we have to use an LLM orchestration framework. For that, I am picking my all-time favourite, Haystack.","Okay, we got what we need. Let’s jump into the real work!"]},{"l":"Loading and Indexing data","p":["Since we have an RAG pipeline, we should build a document indexing service as the first step. For this demo, I am going to use the in-memory vector database that Haystack offers. Please note that each document in our vector database contains,","Content — Which we used to perform a similarity search","Id — Unique identifier","Price — Product price","URL — Product URL","When our RAG pipeline is invoked, the Content field is used for vector search. All other fields are included as metadata. It’s crucial to preserve this metadata as it’s essential for front-end presentation to the user.","Let’s see how we can implement that.","Great, we’ve completed the first step of our AI agent application. Now it’s time to build the product identifier tool. To better understand the primary task of the product identifier, let’s consider the example below.","User Query: I want to buy a camping boot, an charcoal and google pixel 9 back cover. Let’s understand our ideal workflow for product identifier function.","Workflow of product_identifier_function","First, we need to create a tool for analyzing user queries and identifying user-interested products. We can build such a tool using code snippets below."]},{"l":"Building User Query Analyzer","p":["Okay, now we have completed half of our first function, now it’s time to complete the function by adding the RAG pipeline.","Workflow of product_identifier_function"]},{"l":"Creating RAG Pipeline","p":["After this stage, we have completed both RAG and Query Analyzer pipelines. Now it’s time to convert this into a tool. For that, we can use a regular function declaration, as shown below. Creating a tool for the Agent is just like creating a Python function. In case you have a question like,","How is it possible for the Agent to invoke this function?","The answer is straightforward: by leveraging a model-specific tool schema, which we plan to incorporate in a future step. For now, it’s time to create a wrapper function that uses both the query analyzer and RAG pipeline.","Let’s clarify the objectives of this function.","Objective 1: Identify all products the user is interested in and return them as a list. Objective 2: For each identified product, retrieve up to five products from the database along with their metadata."]},{"l":"Finalizing Product Identifier Function","p":["Workflow of product_identifier_function","With that, we have completed our first tool for the agent. Let’s see whether it works as expected.","It worked!! However, it’s worth noting the return output schema. You can see the general schema below.","That’s exactly what we have advised the model to produce in the RAG pipeline. As a next step, let’s build an optional tool called find_budget_friendly_option.","Okay, let's focus on the most crucial aspect of this application, which is enabling the agent to use these functions as needed. As we previously talked about, this is achievable through a model-specific tool schema. Therefore, we need to locate the tool schema specific to the selected model. Fortunately, it's mentioned in the model card here. We need to adjust that to fit our use case."]},{"l":"Finalizing Chat Template","p":["Now there are only a few steps left. Before doing anything, let’s test our agent.","With that, we have completed about 90% of our work.","You almost there","One thing you probably noticed in the above response is that tool calls are enclosed using the XML tag tool_call. Therefore, we have to build some mechanism to extract the tool_call object.","One thing you may have noticed in the above response is that the XML tag tool_call encloses tool calls. Thus, we need to develop a mechanism to extract the tool_call object.","With this step completed, we can directly access the agent’s response when it calls a tool. Now the only thing pending is to get the tool call object and execute the function accordingly. Let’s complete that piece too.","Now it’s time to join each component together and build a proper chat application. I am going to use Gradio for that purpose.","That’s it! We have built the Llama 3-based AI Agent \uD83E\uDD16 with function calling capability. You can access the full code from this GitHub repo. Thanks for reading.","Access to the dataset used in this article is available through this Kaggle link(Under CC0: Public Domain)."]},{"l":"Conclusion","p":["When constructing an AI agent-based system, it's important to consider the time required to complete a task and the number of API calls (tokens) used for each task. One of the major challenges is reducing hallucination in the system, which is an active area of research. Therefore, there are no set rules for building LLMs and agent systems. It's necessary to work patiently and strategically to ensure the AI agent, the LLM, is functioning correctly.","All images, unless otherwise noted, are by the author."]},{"i":"reference","l":"Reference:","p":["Introducing Llama 3.1: Our most capable models to date","Groq's Llama-3-Groq-70B-Tool-Use model","Llama 3 function calling","This article was originally published on Medium."]}],[{"l":"Projects"},{"l":"Daily Llama","p":["Retrieval augmented generation framework based on news contents.","This project presents a retrieval-augmented generation framework tailored for news content. By employing the FAISS index for efficient similarity search, relevant documents are retrieved. Leveraging the LLAMA-2 model, these retrieved documents serve as a foundation for generating comprehensive and contextually accurate answers. Github"]},{"l":"First Ever Sinhala Sentence Transformer Model","p":["Indepedent research project to create the first ever Sinhala Sentence Transformer Model. I was able to create a model that can be used to encode Sinhala sentences into vectors. This model can be used for various NLP tasks such as sentence similarity, sentence classification, etc. Further, I was able to create a BERT model that can be used for Sinhala masked language modeling. For more information, please refer to the HuggingFace space below. Sinhala Embeddings"]},{"l":"Sinhala Zero-Shot Text Classification","p":["Another research project I did to create a zero-shot text classification model for Sinhala. As part of this project, I have trained 3 BERT models with different architectures. Working demo is available in the HuggingFace space below. Sinhala Zero Shot Classification"]}],[{"l":"Reach out to me"},{"l":"Email"}]]